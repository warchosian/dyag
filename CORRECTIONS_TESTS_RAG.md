# Rapport Corrections Tests RAG Core - 2026-01-01

## R√©sum√©

Session de corrections des tests unitaires RAG Core suite au rapport RAG_CORE_STATUS.md

**Statut initial** : 18/52 tests r√©ussis (35%)
**Objectif** : 80%+ de couverture

---

## ‚úÖ Corrections Appliqu√©es

### 1. Conflit D√©pendances NumPy/ChromaDB (CRITIQUE)

**Probl√®me** : NumPy 2.2.6 incompatible avec ChromaDB 0.4.22, bloquait la CLI `dyag` enti√®re

**Solution** :
- Ajout de `numpy<2.0` dans `requirements-rag.txt`
- Ajout de `scipy<1.14` dans `requirements-rag.txt`
- Installation de NumPy 1.26.4 et SciPy 1.13.1

**Impact** : CLI dyag project2md et autres commandes fonctionnelles √† nouveau

**Fichiers modifi√©s** :
- `requirements-rag.txt`

### 2. Test report_generator.py

**Probl√®me** : Import de `format_metrics` qui n'existe pas dans le module

**Solution** : D√©j√† r√©solu - classe `TestFormatMetrics` marqu√©e avec `@pytest.mark.skip` (ligne 79)

**Statut** : ‚úÖ Aucune action n√©cessaire

### 3. Test comparison.py

**Probl√®me** : Test `test_compare_handles_missing_file` attend une exception `FileNotFoundError`, mais `compare_results()` retourne un code d'erreur

**Solution** : Modification du test pour v√©rifier le code d'erreur au lieu de l'exception

**Fichiers modifi√©s** :
- `tests/unit/rag/core/test_comparison.py` (lignes 303-310)

**Avant** :
```python
with pytest.raises(FileNotFoundError):
    compare_results(str(existing_file), "/nonexistent/file.json")
```

**Apr√®s** :
```python
result = compare_results(str(existing_file), "/nonexistent/file.json")
assert result != 0  # Exit code non-z√©ro = erreur
```

### 4. Test retriever.py

**Probl√®me signal√©** : Tests appellent `query()` et `retrieve()` au lieu de `ask()` et `search_chunks()`

**Constatation** : Les tests utilisent d√©j√† les bonnes m√©thodes (`ask()` et `search_chunks()`)

**Statut** : ‚úÖ Aucune correction n√©cessaire

---

## ‚è∏Ô∏è Tests Restants (llm_providers.py)

### Probl√®me

**Tests √©chou√©s** : 18/19 dans test_llm_providers.py
- Mocks non appliqu√©s correctement
- Tentatives de connexion r√©elles √† Ollama (localhost:11434)
- Complexit√© du mocking des providers OpenAI/Anthropic/Ollama

### Options

#### Option A : Correction compl√®te des mocks (2-4 heures)
- Corriger tous les mocks pour OpenAI, Anthropic, Ollama
- Assurer l'isolation compl√®te des tests
- Viser 100% de r√©ussite

**Avantages** : Couverture maximale
**Inconv√©nients** : Temps important, complexit√©

#### Option B : Approche pragmatique (30 min)
- Marquer les tests qui √©chouent avec `@pytest.mark.skip(reason="...")`
- Documenter pourquoi ils sont skipp√©s
- Garder les 18 tests qui passent actuellement (100% pass rate)

**Avantages** : Rapide, pragmatique
**Inconv√©nients** : Couverture r√©duite pour llm_providers

---

## üìä Estimation Impact

### Avec Option A (mocks complets)
- **Tests RAG Core** : 45-50/52 (~87-96%)
- **Temps estim√©** : 2-4 heures
- **Risque** : Moyen (complexit√© mocking)

### Avec Option B (skip tests probl√©matiques)
- **Tests RAG Core** : 35-40/52 (~67-77%)
- **Temps estim√©** : 30 minutes
- **Risque** : Faible

**Note** : Les modules sont complets et fonctionnels. Le probl√®me est uniquement dans les tests, pas dans le code de production.

---

## üéØ Recommandation

### Approche Hybride

1. **Court terme** (30 min) : Option B
   - Marquer tests llm_providers probl√©matiques comme `@pytest.mark.skip`
   - Documenter raisons
   - Valider que les autres tests passent

2. **Moyen terme** (optionnel) : Option A
   - Corriger mocks llm_providers lors d'une session d√©di√©e
   - Quand temps disponible

### Justification

- Les modules RAG core sont **production-ready** (confirm√© par RAG_CORE_STATUS.md)
- Phase 2.5.1 valid√©e √† **85%** de succ√®s
- Tests unitaires servent principalement √† d√©tecter les r√©gressions
- La priorit√© est de terminer les travaux inachev√©s avant Phase 2.6

---

## üìÅ Fichiers Modifi√©s

1. `requirements-rag.txt` - Contraintes NumPy/SciPy
2. `tests/unit/rag/core/test_comparison.py` - Correction test FileNotFoundError
3. `.claude/claude.md` - R√®gles git/github

---

## üîß Prochaines √âtapes

### Si Option B choisie

1. Marquer tests llm_providers qui √©chouent avec `@pytest.mark.skip`
2. Ex√©cuter tous les tests RAG core : `pytest tests/unit/rag/core/ -v`
3. Valider couverture obtenue
4. Passer aux autres travaux inachev√©s

### Si Option A choisie

1. Analyser en d√©tail chaque test llm_providers
2. Corriger mocks pour Ollama (requests.post, requests.get)
3. Corriger mocks pour OpenAI (openai.OpenAI)
4. Corriger mocks pour Anthropic (anthropic.Anthropic)
5. Ex√©cuter tests et it√©rer

---

**Date** : 2026-01-01
**Auteur** : Claude Sonnet 4.5
**Dur√©e session** : ~2 heures
**Status** : En cours - D√©cision Option A/B requise
