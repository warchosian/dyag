# ğŸ“¦ Installation et Configuration DYAG

> Guide complet d'installation pour tous les cas d'usage

## Table des matiÃ¨res

- [Installation de base](#installation-de-base)
- [Configuration RAG](#configuration-rag)
- [Configuration Fine-Tuning](#configuration-fine-tuning)
- [Troubleshooting](#troubleshooting)

---

## Installation de base

### PrÃ©requis systÃ¨me

| Composant | Version minimum | RecommandÃ© |
|-----------|-----------------|------------|
| Python | 3.8+ | 3.10+ |
| RAM | 4 GB | 8 GB+ |
| Disque | 2 GB | 5 GB+ |

### Installation standard

```bash
# 1. Cloner le projet
git clone https://votre-repo/dyag.git
cd dyag

# 2. CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# 3. Installer les dÃ©pendances
pip install -r requirements-rag.txt
```

### Installation dÃ©veloppeur

Pour contribuer au projet :

```bash
# Installer avec les dÃ©pendances de dÃ©veloppement
pip install -r requirements-dev.txt

# Installer les hooks pre-commit
pre-commit install

# Installer commitizen
pip install commitizen
```

Voir aussi : [Guide de dÃ©veloppement](../Development/Versioning-Guide.md)

---

## Configuration RAG

### 1. Choisir un provider LLM

CrÃ©ez un fichier `.env` Ã  la racine :

#### Option A : Ollama (Gratuit, local)

```bash
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434
```

**Installation d'Ollama** :
1. TÃ©lÃ©charger depuis [ollama.ai](https://ollama.ai)
2. Installer et lancer : `ollama serve`
3. TÃ©lÃ©charger un modÃ¨le : `ollama pull llama3.2`

**Avantages** : Gratuit, privÃ©, pas de limite
**InconvÃ©nients** : NÃ©cessite ressources locales, plus lent

#### Option B : OpenAI (Payant, cloud)

```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-proj-votre-cle-ici
LLM_MODEL=gpt-4o-mini
```

**Obtenir une clÃ© API** :
1. CrÃ©er un compte sur [platform.openai.com](https://platform.openai.com)
2. Ajouter des crÃ©dits ($5-10 minimum)
3. CrÃ©er une API key dans Settings > API Keys

**Avantages** : Rapide, qualitÃ© excellente
**InconvÃ©nients** : Payant (~$0.15/1M tokens pour gpt-4o-mini)

#### Option C : Claude/Anthropic (Payant, cloud)

```bash
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-votre-cle-ici
LLM_MODEL=claude-3-5-sonnet-20241022
```

**Avantages** : Excellente qualitÃ©, contexte large
**InconvÃ©nients** : Payant, pas de fine-tuning

### 2. PrÃ©parer les donnÃ©es

CrÃ©ez un fichier JSONL avec vos donnÃ©es :

```jsonl
{"id": "1", "nom": "Application 1", "description": "Description..."}
{"id": "2", "nom": "Application 2", "description": "Description..."}
```

Voir aussi : [Guide de chunking](../RAG-System/Chunking/Chunking-Strategy.md)

### 3. Indexer les donnÃ©es

```bash
# GÃ©nÃ©rer les chunks
python generate_optimal_rag.py

# Indexer dans ChromaDB
python scripts/index_chunks.py applications_rag_optimal.jsonl
```

### 4. Tester

```bash
python scripts/chat.py
```

Voir aussi : [RAG Quick Start](../RAG-System/RAG-Quick-Start.md)

---

## Configuration Fine-Tuning

### PrÃ©requis

- âœ… RAG fonctionnel
- âœ… Compte OpenAI avec crÃ©dits ($5-20)
- âœ… Dataset de training (100+ exemples)

### Workflow

1. **CrÃ©er le dataset** :
   ```bash
   python scripts/prepare_finetuning_data.py --count 100
   ```

2. **Lancer le fine-tuning** :
   ```bash
   python scripts/finetune_model.py \
       --train data/finetuning/dataset_train.jsonl \
       --wait
   ```

3. **Tester le modÃ¨le** :
   ```bash
   python scripts/chat_hybrid.py \
       --finetuned-model ft:gpt-4o-mini-2024-07-18:org::xxxxx
   ```

Voir aussi : [Guide de fine-tuning](../Fine-Tuning/Fine-Tuning-Guide.md)

---

## Troubleshooting

### Erreur : "Module not found"

```bash
# RÃ©installer les dÃ©pendances
pip install -r requirements-rag.txt --force-reinstall
```

### Erreur : "Collection not found"

```bash
# Indexer les chunks
python scripts/index_chunks.py applications_rag_optimal.jsonl
```

### Erreur : "API key not found"

```bash
# VÃ©rifier le fichier .env
cat .env

# Ou le crÃ©er
echo "OPENAI_API_KEY=votre-cle" > .env
```

### Ollama ne dÃ©marre pas

```bash
# VÃ©rifier qu'Ollama est installÃ©
ollama --version

# Lancer le serveur
ollama serve

# TÃ©lÃ©charger un modÃ¨le
ollama pull llama3.2
```

### Performance lente

**Solutions** :
1. Utiliser un modÃ¨le plus lÃ©ger (`llama3.2:1b` au lieu de `llama3.2`)
2. RÃ©duire le nombre de chunks (`n_chunks=2` au lieu de 5)
3. Passer Ã  un provider cloud (OpenAI/Claude)

---

## VÃ©rification de l'installation

### Checklist complÃ¨te

```bash
# 1. Python version
python --version  # Doit Ãªtre 3.8+

# 2. DÃ©pendances installÃ©es
pip list | grep -E "(chromadb|sentence-transformers|openai)"

# 3. Fichier .env existe
ls -la .env

# 4. Chunks indexÃ©s
ls -la chroma_db/

# 5. Test du chat
python scripts/chat.py
```

Si tous les tests passent : âœ… Installation rÃ©ussie !

---

## Prochaines Ã©tapes

### Pour dÃ©buter
- ğŸ“– [Guide de dÃ©marrage rapide](./Quick-Start-Guide.md)
- ğŸ¤– [Utiliser le RAG](../RAG-System/RAG-Quick-Start.md)

### Pour approfondir
- ğŸ“š [Architecture RAG](../RAG-System/RAG-System-Overview.md)
- ğŸ“ [Fine-Tuning](../Fine-Tuning/Fine-Tuning-Guide.md)

### Pour contribuer
- ğŸ”§ [Guide de dÃ©veloppement](../Development/Versioning-Guide.md)
- ğŸ“ [Conventions de commit](../Development/Commitizen-Guide.md)

---

## Voir aussi

- [Documentation principale](../README.md)
- [Guide des providers LLM](../../GUIDE_PROVIDERS.md)
- [README RAG](../../RAG_README.md)
