# Guide Complet : Fine-tuning de ModÃ¨les LLM avec Dyag

Ce guide vous accompagne pas-Ã -pas dans le fine-tuning de modÃ¨les de langage (LLM) Ã  partir de vos donnÃ©es d'applications, en utilisant Dyag. ComplÃ©mentaire au guide RAG, il vous permet de choisir la meilleure approche pour votre cas d'usage.

## ğŸ“‹ Table des matiÃ¨res

- [Introduction](#introduction)
- [RAG vs Fine-tuning : Quelle approche choisir ?](#rag-vs-fine-tuning--quelle-approche-choisir-)
- [PrÃ©requis](#prÃ©requis)
- [Vue d'ensemble du workflow](#vue-densemble-du-workflow)
- [Ã‰tape 1 : PrÃ©paration des donnÃ©es d'entraÃ®nement](#Ã©tape-1--prÃ©paration-des-donnÃ©es-dentraÃ®nement)
- [Ã‰tape 2 : CrÃ©ation du dataset de fine-tuning](#Ã©tape-2--crÃ©ation-du-dataset-de-fine-tuning)
- [Ã‰tape 3 : Validation et nettoyage du dataset](#Ã©tape-3--validation-et-nettoyage-du-dataset)
- [Ã‰tape 4 : Fine-tuning avec OpenAI](#Ã©tape-4--fine-tuning-avec-openai)
- [Ã‰tape 5 : Fine-tuning local avec Ollama/LoRA](#Ã©tape-5--fine-tuning-local-avec-ollama-lora)
- [Ã‰tape 6 : Ã‰valuation du modÃ¨le fine-tunÃ©](#Ã©tape-6--Ã©valuation-du-modÃ¨le-fine-tunÃ©)
- [Ã‰tape 7 : Comparaison RAG vs Fine-tuning](#Ã©tape-7--comparaison-rag-vs-fine-tuning)
- [Annexes](#annexes)

---

## Introduction

Le **fine-tuning** consiste Ã  adapter un modÃ¨le prÃ©-entraÃ®nÃ© (GPT, Llama, etc.) Ã  vos donnÃ©es spÃ©cifiques. Contrairement au RAG qui rÃ©cupÃ¨re des informations, le fine-tuning **encode les connaissances directement dans les poids du modÃ¨le**.

### Quand utiliser le fine-tuning ?

âœ… **Utilisez le fine-tuning si :**
- Vous avez **beaucoup de donnÃ©es** (>1000 exemples)
- Les donnÃ©es changent **rarement** (mise Ã  jour trimestrielle/annuelle)
- Vous voulez un **style de rÃ©ponse spÃ©cifique** (ton, format)
- Vous avez besoin de **latence trÃ¨s faible** (pas de rÃ©cupÃ©ration)
- Vous avez un **budget** pour l'entraÃ®nement

âŒ **N'utilisez PAS le fine-tuning si :**
- Vos donnÃ©es changent **frÃ©quemment** (quotidien/hebdomadaire)
- Vous avez **peu de donnÃ©es** (<500 exemples)
- Vous voulez **mettre Ã  jour facilement** les connaissances
- Vous avez un **budget limitÃ©**
- Vous voulez **tracer les sources** des rÃ©ponses

---

## RAG vs Fine-tuning : Quelle approche choisir ?

| CritÃ¨re | RAG | Fine-tuning | Recommandation |
|---------|-----|-------------|----------------|
| **DonnÃ©es** | 50+ documents | 1000+ exemples QA | RAG pour petits corpus |
| **Mise Ã  jour** | InstantanÃ©e | RÃ©-entraÃ®nement complet | RAG pour donnÃ©es dynamiques |
| **CoÃ»t** | ~$0/mois (Ollama) | $50-500/entraÃ®nement | RAG pour budget limitÃ© |
| **Latence** | 2-5s | 0.5-2s | Fine-tuning si latence critique |
| **TraÃ§abilitÃ©** | âœ… Sources citÃ©es | âŒ BoÃ®te noire | RAG pour audit/compliance |
| **Style** | DÃ©pend du prompt | âœ… Appris | Fine-tuning pour style spÃ©cifique |
| **PrÃ©cision** | 80-90% | 85-95% | Fine-tuning si donnÃ©es suffisantes |

### ğŸ’¡ Approche hybride (recommandÃ©e)

La meilleure approche combine souvent les deux :
1. **Fine-tuning** pour le style et les connaissances de base
2. **RAG** pour les dÃ©tails spÃ©cifiques et les mises Ã  jour frÃ©quentes

Exemple : ModÃ¨le fine-tunÃ© sur le jargon du domaine + RAG pour les applications spÃ©cifiques.

---

## PrÃ©requis

### Installation

```bash
# Installation de base
poetry install

# Installation avec support RAG/Fine-tuning
poetry install -E rag
pip install -r requirements-finetuning.txt
```

### Outils requis

| Outil | Usage | Installation |
|-------|-------|--------------|
| **OpenAI API** | Fine-tuning cloud (payant) | ClÃ© API OpenAI |
| **Ollama** | Fine-tuning local (gratuit) | https://ollama.com |
| **unsloth** | AccÃ©lÃ©ration LoRA (optionnel) | `pip install unsloth` |
| **wandb** | Monitoring (optionnel) | `pip install wandb` |

### Configuration

```bash
# CrÃ©er .env
cat > .env << EOF
# Fine-tuning OpenAI
OPENAI_API_KEY=sk-proj-votre-clÃ©

# Fine-tuning local
WANDB_API_KEY=votre-clÃ©-wandb  # Optionnel

# ModÃ¨le de base pour fine-tuning local
BASE_MODEL=llama3.2
EOF
```

---

## Vue d'ensemble du workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WORKFLOW FINE-TUNING COMPLET                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ğŸ“Š PRÃ‰PARATION DES DONNÃ‰ES
   â””â”€> Analyser applicationsIA_mini_normalized.json
       - Identifier les champs clÃ©s
       - Comprendre la structure

2. ğŸ”¨ CRÃ‰ATION DU DATASET
   â””â”€> dyag create_rag
       - GÃ©nÃ©ration automatique de questions/rÃ©ponses
       - Format OpenAI compatible (.jsonl)
       - Validation de la qualitÃ©

3. âœ… VALIDATION & NETTOYAGE
   â””â”€> dyag analyze_training
       - VÃ©rification du format
       - DÃ©tection d'anomalies
       - Statistiques du dataset

4. â˜ï¸ FINE-TUNING OPENAI
   â””â”€> Utiliser l'API OpenAI
       - Upload du dataset
       - Lancement de l'entraÃ®nement
       - Monitoring des mÃ©triques

5. ğŸ’» FINE-TUNING LOCAL (LORA)
   â””â”€> Ollama + LoRA
       - Configuration de l'environnement
       - EntraÃ®nement adaptÃ©
       - Export du modÃ¨le

6. ğŸ“ˆ Ã‰VALUATION
   â””â”€> dyag evaluate_rag (avec modÃ¨le fine-tunÃ©)
       - MÃ©triques de performance
       - Comparaison avec baseline
       - Analyse des erreurs

7. ğŸ”„ AMÃ‰LIORATION ITÃ‰RATIVE
   â””â”€> Cycle d'optimisation
       - Ajuster hyperparamÃ¨tres
       - Enrichir le dataset
       - Re-tester
```

---

## Ã‰tape 1 : PrÃ©paration des donnÃ©es d'entraÃ®nement

### 1.1 Analyse du fichier source

**Commande CLI :**
```bash
# Examiner la structure
python -c "
import json
data = json.load(open('examples/test-mygusi/applicationsIA_mini_normalized.json'))
apps = data['applicationsia mini']

print(f'ğŸ“Š Statistiques:')
print(f'  Nombre total d\\'applications: {len(apps)}')
print(f'  Applications en production: {sum(1 for a in apps if a.get(\"statut si\") == \"en production\")}')
print(f'  Applications avec descriptif: {sum(1 for a in apps if a.get(\"descriptif\"))}')

# Analyser les champs disponibles
all_fields = set()
for app in apps:
    all_fields.update(app.keys())

print(f'\\nğŸ“‹ Champs disponibles ({len(all_fields)}):')
for field in sorted(all_fields):
    count = sum(1 for a in apps if field in a and a[field])
    print(f'  - {field}: {count}/{len(apps)} ({count/len(apps)*100:.0f}%)')
"
```

**RÃ©sultat attendu :**
```
ğŸ“Š Statistiques:
  Nombre total d'applications: 45
  Applications en production: 28
  Applications avec descriptif: 42

ğŸ“‹ Champs disponibles (15):
  - nom: 45/45 (100%)
  - nom long: 43/45 (96%)
  - descriptif: 42/45 (93%)
  - statut si: 45/45 (100%)
  - domaines et sous domaines: 40/45 (89%)
  - sites: 38/45 (84%)
  - acteurs: 35/45 (78%)
  - contacts: 30/45 (67%)
  ...
```

### 1.2 Identifier les patterns de questions

Pour un bon dataset de fine-tuning, il faut couvrir diffÃ©rents types de questions :

| Type de question | Exemple | Champs utilisÃ©s |
|------------------|---------|-----------------|
| **DÃ©finition** | "Qu'est-ce que {nom} ?" | nom, nom long, descriptif |
| **Statut** | "Quel est le statut de {nom} ?" | nom, statut si |
| **Domaine** | "Dans quel domaine est {nom} ?" | nom, domaines et sous domaines |
| **Acteurs** | "Qui gÃ¨re {nom} ?" | nom, acteurs |
| **Sites** | "OÃ¹ trouver {nom} ?" | nom, sites |
| **Comparative** | "DiffÃ©rence entre {nom1} et {nom2} ?" | Tous |
| **Listing** | "Liste des applications {critÃ¨re}" | statut si, domaines |

### 1.3 Calculer la taille nÃ©cessaire du dataset

**RÃ¨gle gÃ©nÃ©rale :**
- **Minimum viable** : 100 exemples (pour fine-tuning lÃ©ger)
- **Bon** : 500-1000 exemples
- **Optimal** : 2000-5000 exemples
- **Enterprise** : 10000+ exemples

**Pour notre cas (45 applications) :**
```
Nombre d'exemples = Nb applications Ã— Nb types de questions Ã— Variations

Exemple :
45 apps Ã— 7 types Ã— 3 variations = 945 exemples
```

âœ… C'est suffisant pour un bon fine-tuning !

---

## Ã‰tape 2 : CrÃ©ation du dataset de fine-tuning

### 2.1 GÃ©nÃ©ration automatique avec dyag

**Commande CLI :**
```bash
# CrÃ©er le dataset de fine-tuning
dyag create_rag \
  examples/test-mygusi/applicationsIA_mini_normalized.json \
  --output training/finetuning_dataset.jsonl \
  --num-questions 1000 \
  --question-types all \
  --format openai \
  --add-negatives \
  --verbose
```

**ParamÃ¨tres expliquÃ©s :**
- `--num-questions 1000` : GÃ©nÃ©rer 1000 paires question/rÃ©ponse
- `--question-types all` : Tous les types (dÃ©finition, statut, domaine, etc.)
- `--format openai` : Format compatible OpenAI fine-tuning API
- `--add-negatives` : Ajouter des exemples nÃ©gatifs (questions sans rÃ©ponse)
- `--verbose` : Afficher la progression

**Format OpenAI gÃ©nÃ©rÃ© :**
```json
{"messages": [{"role": "system", "content": "Tu es un assistant expert en applications du systÃ¨me d'information franÃ§ais."}, {"role": "user", "content": "Qu'est-ce que l'application 6tzen ?"}, {"role": "assistant", "content": "L'application 6tzen est l'outil national de dÃ©matÃ©rialisation des dÃ©marches des transports routiers. Elle est actuellement en production et permet aux entreprises de transport routier d'effectuer leurs dÃ©marches administratives en ligne."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant expert en applications du systÃ¨me d'information franÃ§ais."}, {"role": "user", "content": "Quel est le statut de 8-SINP ?"}, {"role": "assistant", "content": "L'application 8-SINP (plateformes rÃ©gionales SINP habilitÃ©es) est actuellement en construction."}]}
```

### 2.2 Split train/validation/test

**Commande CLI :**
```bash
# SÃ©parer en train (80%), validation (10%), test (10%)
python -c "
import json
import random

# Charger le dataset
lines = open('training/finetuning_dataset.jsonl').readlines()
random.shuffle(lines)

# Split
n = len(lines)
train_size = int(0.8 * n)
val_size = int(0.1 * n)

train = lines[:train_size]
val = lines[train_size:train_size+val_size]
test = lines[train_size+val_size:]

# Sauvegarder
with open('training/train.jsonl', 'w') as f:
    f.writelines(train)
with open('training/validation.jsonl', 'w') as f:
    f.writelines(val)
with open('training/test.jsonl', 'w') as f:
    f.writelines(test)

print(f'âœ… Dataset sÃ©parÃ©:')
print(f'  Train: {len(train)} exemples')
print(f'  Validation: {len(val)} exemples')
print(f'  Test: {len(test)} exemples')
"
```

**RÃ©sultat attendu :**
```
âœ… Dataset sÃ©parÃ©:
  Train: 800 exemples
  Validation: 100 exemples
  Test: 100 exemples
```

---

## Ã‰tape 3 : Validation et nettoyage du dataset

### 3.1 Analyse de la couverture

**Commande CLI :**
```bash
# Analyser la couverture des applications
dyag analyze_training \
  examples/test-mygusi/applicationsIA_mini_normalized.json \
  training/train.jsonl \
  --verbose
```

**RÃ©sultat attendu :**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ANALYSE DE COUVERTURE DU TRAINING               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Statistiques globales:
  Applications totales: 45
  Applications couvertes: 43 (95.6%)
  Applications non couvertes: 2
    - ID 1523: Application XYZ
    - ID 1789: Application ABC

ğŸ“ˆ Distribution des types de questions:
  DÃ©finition: 215 (26.9%)
  Statut: 180 (22.5%)
  Domaine: 165 (20.6%)
  Acteurs: 120 (15.0%)
  Sites: 80 (10.0%)
  Comparative: 40 (5.0%)

âš ï¸  Recommandations:
  [1] Ajouter des exemples pour les applications non couvertes
  [2] Ã‰quilibrer les types de questions (plus de comparatives)
  [3] VÃ©rifier la qualitÃ© des descriptifs trop courts (<50 caractÃ¨res)
```

### 3.2 Validation du format

**Commande CLI :**
```bash
# Valider le format OpenAI
python -c "
import json

errors = []
for i, line in enumerate(open('training/train.jsonl'), 1):
    try:
        data = json.loads(line)

        # VÃ©rifier la structure
        if 'messages' not in data:
            errors.append(f'Ligne {i}: Pas de clÃ© \"messages\"')
            continue

        messages = data['messages']
        if len(messages) < 2:
            errors.append(f'Ligne {i}: Moins de 2 messages')

        # VÃ©rifier les rÃ´les
        roles = [m['role'] for m in messages]
        if 'user' not in roles or 'assistant' not in roles:
            errors.append(f'Ligne {i}: RÃ´les manquants')

    except json.JSONDecodeError:
        errors.append(f'Ligne {i}: JSON invalide')

if errors:
    print(f'âŒ {len(errors)} erreurs trouvÃ©es:')
    for err in errors[:10]:  # Afficher les 10 premiÃ¨res
        print(f'  {err}')
else:
    print('âœ… Format valide! Aucune erreur dÃ©tectÃ©e.')
"
```

### 3.3 Nettoyage des duplicatas

**Commande CLI :**
```bash
# DÃ©tecter et supprimer les duplicatas
python -c "
import json
from collections import defaultdict

seen = set()
unique_lines = []
duplicates = 0

for line in open('training/train.jsonl'):
    data = json.loads(line)
    # CrÃ©er une clÃ© unique basÃ©e sur la question
    key = data['messages'][1]['content']  # Question de l'user

    if key not in seen:
        seen.add(key)
        unique_lines.append(line)
    else:
        duplicates += 1

# Sauvegarder la version nettoyÃ©e
with open('training/train_clean.jsonl', 'w') as f:
    f.writelines(unique_lines)

print(f'âœ… Nettoyage terminÃ©:')
print(f'  Lignes originales: {len(unique_lines) + duplicates}')
print(f'  Duplicatas supprimÃ©s: {duplicates}')
print(f'  Lignes uniques: {len(unique_lines)}')
"
```

---

## Ã‰tape 4 : Fine-tuning avec OpenAI

### 4.1 VÃ©rifier les prÃ©requis

```bash
# Installer OpenAI CLI
pip install openai

# VÃ©rifier la clÃ© API
python -c "
import os
from openai import OpenAI

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
print('âœ… ClÃ© API OpenAI valide')
"
```

### 4.2 Upload du dataset

```bash
# Upload training file
python -c "
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Upload train
train_file = client.files.create(
    file=open('training/train_clean.jsonl', 'rb'),
    purpose='fine-tune'
)
print(f'ğŸ“¤ Train file uploaded: {train_file.id}')

# Upload validation
val_file = client.files.create(
    file=open('training/validation.jsonl', 'rb'),
    purpose='fine-tune'
)
print(f'ğŸ“¤ Validation file uploaded: {val_file.id}')

# Sauvegarder les IDs
with open('training/file_ids.txt', 'w') as f:
    f.write(f'TRAIN_FILE_ID={train_file.id}\\n')
    f.write(f'VAL_FILE_ID={val_file.id}\\n')
"
```

### 4.3 Lancer le fine-tuning

```bash
# CrÃ©er le job de fine-tuning
python -c "
from openai import OpenAI
import os

# Lire les IDs
with open('training/file_ids.txt') as f:
    ids = dict(line.strip().split('=') for line in f)

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# CrÃ©er le fine-tuning job
job = client.fine_tuning.jobs.create(
    training_file=ids['TRAIN_FILE_ID'],
    validation_file=ids['VAL_FILE_ID'],
    model='gpt-3.5-turbo',  # ou 'gpt-4o-mini-2024-07-18'
    hyperparameters={
        'n_epochs': 3,
        'batch_size': 4,
        'learning_rate_multiplier': 2.0
    }
)

print(f'ğŸš€ Fine-tuning job crÃ©Ã©: {job.id}')
print(f'   ModÃ¨le de base: {job.model}')
print(f'   Status: {job.status}')

# Sauvegarder le job ID
with open('training/job_id.txt', 'w') as f:
    f.write(job.id)
"
```

**CoÃ»t estimÃ© :**
- GPT-3.5-turbo : ~$8 pour 800 exemples Ã— 3 epochs
- GPT-4o-mini : ~$15 pour 800 exemples Ã— 3 epochs

### 4.4 Monitoring du fine-tuning

```bash
# Surveiller la progression
python -c "
from openai import OpenAI
import os
import time

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Lire le job ID
job_id = open('training/job_id.txt').read().strip()

print('ğŸ“Š Monitoring du fine-tuning (Ctrl+C pour arrÃªter)\\n')

while True:
    job = client.fine_tuning.jobs.retrieve(job_id)

    print(f'Status: {job.status}')
    if job.status == 'succeeded':
        print(f'âœ… Fine-tuning terminÃ©!')
        print(f'   ModÃ¨le fine-tunÃ©: {job.fine_tuned_model}')

        # Sauvegarder le modÃ¨le ID
        with open('training/finetuned_model.txt', 'w') as f:
            f.write(job.fine_tuned_model)
        break
    elif job.status in ['failed', 'cancelled']:
        print(f'âŒ Fine-tuning Ã©chouÃ©: {job.status}')
        break

    time.sleep(30)  # VÃ©rifier toutes les 30s
"
```

### 4.5 Tester le modÃ¨le fine-tunÃ©

```bash
# Tester avec quelques questions
python -c "
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Lire le modÃ¨le fine-tunÃ©
model_id = open('training/finetuned_model.txt').read().strip()

print(f'ğŸ§ª Test du modÃ¨le fine-tunÃ©: {model_id}\\n')

questions = [
    'Qu\\'est-ce que l\\'application 6tzen ?',
    'Quel est le statut de 8-SINP ?',
    'Liste les applications du domaine biodiversitÃ©'
]

for q in questions:
    response = client.chat.completions.create(
        model=model_id,
        messages=[
            {'role': 'system', 'content': 'Tu es un assistant expert en applications SI.'},
            {'role': 'user', 'content': q}
        ]
    )

    print(f'Q: {q}')
    print(f'R: {response.choices[0].message.content}\\n')
"
```

---

## Ã‰tape 5 : Fine-tuning local avec Ollama/LoRA

### 5.1 PrÃ©parer l'environnement

```bash
# Installer les dÃ©pendances
pip install unsloth transformers datasets peft bitsandbytes

# VÃ©rifier GPU (optionnel mais recommandÃ©)
nvidia-smi
```

### 5.2 Convertir le dataset au format Alpaca

```python
# Script de conversion : convert_to_alpaca.py
import json

def convert_to_alpaca(input_file, output_file):
    """Convertit du format OpenAI au format Alpaca."""
    alpaca_data = []

    for line in open(input_file):
        data = json.loads(line)
        messages = data['messages']

        # Extraire instruction et rÃ©ponse
        instruction = messages[1]['content']  # user message
        output = messages[2]['content']       # assistant message
        system = messages[0]['content'] if messages[0]['role'] == 'system' else ''

        alpaca_data.append({
            'instruction': instruction,
            'input': system,
            'output': output
        })

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(alpaca_data, f, ensure_ascii=False, indent=2)

    print(f'âœ… Converti {len(alpaca_data)} exemples vers {output_file}')

# Convertir
convert_to_alpaca('training/train_clean.jsonl', 'training/train_alpaca.json')
convert_to_alpaca('training/validation.jsonl', 'training/val_alpaca.json')
```

### 5.3 Fine-tuning avec unsloth (LoRA)

```python
# Script : finetune_local.py
from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

# 1. Charger le modÃ¨le de base
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3.2-3b",  # ou "unsloth/mistral-7b"
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,  # Quantization 4-bit pour Ã©conomiser la RAM
)

# 2. Configurer LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Rank LoRA
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
)

# 3. Charger le dataset
dataset = load_dataset("json", data_files={
    "train": "training/train_alpaca.json",
    "validation": "training/val_alpaca.json"
})

# 4. CrÃ©er le trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    dataset_text_field="output",
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=500,  # Ajuster selon la taille du dataset
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        output_dir="models/finetuned_llama",
        optim="adamw_8bit",
        save_strategy="steps",
        save_steps=100,
        evaluation_strategy="steps",
        eval_steps=100,
    ),
)

# 5. Lancer l'entraÃ®nement
print("ğŸš€ DÃ©marrage du fine-tuning local...")
trainer.train()

# 6. Sauvegarder le modÃ¨le
model.save_pretrained("models/finetuned_llama_final")
tokenizer.save_pretrained("models/finetuned_llama_final")
print("âœ… ModÃ¨le sauvegardÃ© dans models/finetuned_llama_final")
```

**Lancer :**
```bash
python finetune_local.py
```

**Temps estimÃ© :**
- GPU RTX 3090 : 30-60 minutes
- GPU T4 (Colab) : 1-2 heures
- CPU : Non recommandÃ© (>10 heures)

### 5.4 Export vers Ollama

```bash
# CrÃ©er un Modelfile pour Ollama
cat > Modelfile.applications << 'EOF'
FROM models/finetuned_llama_final

PARAMETER temperature 0.7
PARAMETER top_p 0.9

SYSTEM """
Tu es un assistant expert en applications du systÃ¨me d'information franÃ§ais.
Tu rÃ©ponds de maniÃ¨re prÃ©cise et concise aux questions sur les applications.
"""
EOF

# CrÃ©er le modÃ¨le Ollama
ollama create applications-ia -f Modelfile.applications

# Tester
ollama run applications-ia "Qu'est-ce que 6tzen ?"
```

---

## Ã‰tape 6 : Ã‰valuation du modÃ¨le fine-tunÃ©

### 6.1 Ã‰valuation automatique avec dyag

**Commande CLI (OpenAI) :**
```bash
# Tester avec le modÃ¨le fine-tunÃ© OpenAI
export FINETUNED_MODEL=$(cat training/finetuned_model.txt)

python -c "
import os
os.environ['LLM_PROVIDER'] = 'openai'
os.environ['LLM_MODEL'] = os.environ['FINETUNED_MODEL']
" && dyag evaluate_rag \
  training/test.jsonl \
  --collection applications_ia \
  --output evaluation/finetuned_results.json \
  --verbose
```

**Commande CLI (Ollama local) :**
```bash
# Tester avec le modÃ¨le Ollama fine-tunÃ©
export LLM_PROVIDER=ollama
export LLM_MODEL=applications-ia

dyag evaluate_rag \
  training/test.jsonl \
  --collection applications_ia \
  --output evaluation/finetuned_local_results.json \
  --verbose
```

### 6.2 Comparaison baseline vs fine-tuned

```bash
# Comparer les rÃ©sultats
python -c "
import json

# Charger les rÃ©sultats
baseline = json.load(open('evaluation/baseline_results.json'))  # RAG ou modÃ¨le non fine-tunÃ©
finetuned = json.load(open('evaluation/finetuned_results.json'))

print('ğŸ“Š Comparaison Baseline vs Fine-tuned\\n')
print(f'{\"MÃ©trique\":<25} {\"Baseline\":>12} {\"Fine-tuned\":>12} {\"Î”\":>8}')
print('-' * 60)

metrics = ['accuracy', 'precision', 'recall', 'f1_score']
for metric in metrics:
    b_val = baseline.get(metric, 0)
    f_val = finetuned.get(metric, 0)
    delta = f_val - b_val

    print(f'{metric.capitalize():<25} {b_val:>11.1%} {f_val:>11.1%} {delta:>+7.1%}')

print('\\nâ±ï¸  Latence')
b_time = baseline.get('average_generation_time_ms', 0)
f_time = finetuned.get('average_generation_time_ms', 0)
print(f'Baseline: {b_time:.0f}ms')
print(f'Fine-tuned: {f_time:.0f}ms')
print(f'AmÃ©lioration: {(b_time-f_time)/b_time*100:+.1f}%')
"
```

**RÃ©sultat attendu :**
```
ğŸ“Š Comparaison Baseline vs Fine-tuned

MÃ©trique                      Baseline   Fine-tuned        Î”
------------------------------------------------------------
Accuracy                         82.0%        91.5%    +9.5%
Precision                        78.5%        89.2%   +10.7%
Recall                           75.3%        88.8%   +13.5%
F1_score                         76.9%        89.0%   +12.1%

â±ï¸  Latence
Baseline: 2340ms
Fine-tuned: 1850ms
AmÃ©lioration: +20.9%
```

### 6.3 Analyse qualitative

```bash
# Examiner les prÃ©dictions sur le test set
python -c "
import json

results = json.load(open('evaluation/finetuned_results.json'))
errors = results.get('errors', [])

print(f'âŒ Erreurs ({len(errors)}) :\\n')
for i, err in enumerate(errors[:5], 1):  # Top 5
    print(f'{i}. Question: {err[\"question\"]}')
    print(f'   Attendu: {err[\"expected\"][:100]}...')
    print(f'   Obtenu: {err[\"got\"][:100]}...')
    print(f'   Raison: {err.get(\"reason\", \"N/A\")}\\n')
"
```

---

## Ã‰tape 7 : Comparaison RAG vs Fine-tuning

### 7.1 Test cÃ´te Ã  cÃ´te

```bash
# CrÃ©er un script de comparaison
cat > compare_approaches.py << 'EOF'
from dyag.rag_query import RAGQuerySystem
from openai import OpenAI
import os

questions = [
    "Qu'est-ce que l'application 6tzen ?",
    "Quel est le statut de 8-SINP ?",
    "Liste les applications du domaine biodiversitÃ©"
]

# RAG
print("ğŸ” RAG (avec Ollama)")
rag = RAGQuerySystem(collection_name='applications_ia')
for q in questions:
    result = rag.query(q, n_chunks=5)
    print(f"Q: {q}")
    print(f"R: {result['answer'][:200]}...\n")

# Fine-tuned
print("\nğŸ¯ Fine-tuned (OpenAI)")
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
model = open('training/finetuned_model.txt').read().strip()

for q in questions:
    response = client.chat.completions.create(
        model=model,
        messages=[{'role': 'user', 'content': q}]
    )
    print(f"Q: {q}")
    print(f"R: {response.choices[0].message.content[:200]}...\n")
EOF

python compare_approaches.py
```

### 7.2 Tableau de dÃ©cision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              QUAND UTILISER QUELLE APPROCHE ?                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Votre situation                    â”‚ Recommandation            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ <50 applications                 â”‚ âŒ NI l'un NI l'autre     â”‚
â”‚ â€¢ Peu de questions                 â”‚ â†’ Utiliser GPT-4 direct   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 50-200 applications              â”‚ âœ… RAG                    â”‚
â”‚ â€¢ Mises Ã  jour frÃ©quentes          â”‚                           â”‚
â”‚ â€¢ Budget limitÃ©                    â”‚                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 200-1000 applications            â”‚ âœ… RAG + Fine-tuning      â”‚
â”‚ â€¢ Style spÃ©cifique requis          â”‚ â†’ Fine-tuning pour style  â”‚
â”‚ â€¢ Budget moyen                     â”‚ â†’ RAG pour connaissances  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ >1000 applications               â”‚ âœ… Fine-tuning            â”‚
â”‚ â€¢ DonnÃ©es stables                  â”‚ â†’ Avec RAG optionnel      â”‚
â”‚ â€¢ Latence critique                 â”‚ â†’ pour mises Ã  jour       â”‚
â”‚ â€¢ Budget Ã©levÃ©                     â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Annexes

### Annexe A : HyperparamÃ¨tres recommandÃ©s

#### OpenAI Fine-tuning

| ParamÃ¨tre | Petit dataset (<500) | Moyen (500-2000) | Grand (>2000) |
|-----------|---------------------|------------------|---------------|
| **n_epochs** | 5-10 | 3-5 | 1-3 |
| **batch_size** | 2-4 | 4-8 | 8-16 |
| **learning_rate_multiplier** | 2.0 | 1.5 | 1.0 |

#### LoRA Local

| ParamÃ¨tre | Petit dataset | Moyen | Grand |
|-----------|--------------|--------|-------|
| **r** (rank) | 8 | 16 | 32 |
| **lora_alpha** | 16 | 32 | 64 |
| **lora_dropout** | 0.1 | 0.05 | 0.01 |
| **learning_rate** | 3e-4 | 2e-4 | 1e-4 |

### Annexe B : Checklist qualitÃ© du dataset

Avant de lancer le fine-tuning, vÃ©rifiez :

- [ ] **Taille minimale** : >100 exemples (idÃ©al >500)
- [ ] **Format valide** : Tous les JSONs sont bien formÃ©s
- [ ] **Pas de duplicatas** : Questions uniques
- [ ] **Couverture** : >90% des applications reprÃ©sentÃ©es
- [ ] **Ã‰quilibre** : Chaque type de question reprÃ©sentÃ©
- [ ] **QualitÃ©** : RÃ©ponses prÃ©cises et complÃ¨tes
- [ ] **Longueur** : RÃ©ponses ni trop courtes (<20 mots) ni trop longues (>200 mots)
- [ ] **NÃ©gatifs** : Inclut des exemples "Je ne sais pas"
- [ ] **Split** : Train/Val/Test sÃ©parÃ©s (80/10/10)

### Annexe C : CoÃ»ts estimÃ©s (2025)

| Provider | Setup | Training (1000 exemples) | Inference |
|----------|-------|-------------------------|-----------|
| **Ollama** | Gratuit | Gratuit (GPU local) | Gratuit |
| **OpenAI GPT-3.5-turbo** | $0 | ~$8 | ~$0.002/requÃªte |
| **OpenAI GPT-4o-mini** | $0 | ~$15 | ~$0.005/requÃªte |
| **Anthropic Claude** | N/A | Non disponible | ~$0.015/requÃªte |

### Annexe D : Ressources

**Fine-tuning OpenAI :**
- https://platform.openai.com/docs/guides/fine-tuning

**Unsloth (LoRA rapide) :**
- https://github.com/unslothai/unsloth

**Datasets :**
- https://huggingface.co/datasets

**Monitoring :**
- https://wandb.ai/

---

## ğŸ¯ Checklist de succÃ¨s

Votre modÃ¨le fine-tunÃ© est prÃªt si :

- [ ] **Accuracy > 85%** sur le test set
- [ ] **Latence < 2s** par requÃªte
- [ ] **Pas d'hallucinations** majeures
- [ ] **Style cohÃ©rent** avec vos attentes
- [ ] **CoÃ»t acceptable** (training + inference)
- [ ] **DÃ©ployable** (API accessible ou modÃ¨le local)
- [ ] **Maintenable** (process de rÃ©-entraÃ®nement documentÃ©)

---

**DerniÃ¨re mise Ã  jour** : 2025-01-16
**Version du guide** : 1.0
**Auteur** : Ã‰quipe Dyag

---

**Dyag** - Fine-tuning de qualitÃ©, Ã©tape par Ã©tape! ğŸ¯ğŸš€
