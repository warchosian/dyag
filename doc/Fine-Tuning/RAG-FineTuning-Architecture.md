# Architecture : Cohabitation RAG et Fine-Tuning

## Executive Summary

**OUI**, RAG et fine-tuning peuvent et **doivent** cohabiter sur ce projet. Ils sont **compl√©mentaires** :

- **Fine-tuning** : Adapte le mod√®le LLM au domaine/style de vos applications
- **RAG** : Injecte des informations factuelles sp√©cifiques et √† jour dans le contexte

**R√©sultat optimal** = Fine-tuning du mod√®le + RAG pour les faits sp√©cifiques

## 1. Comparaison RAG vs Fine-Tuning

### RAG (Retrieval Augmented Generation)
‚úÖ **Avantages**
- Mise √† jour facile (ajouter des chunks)
- Pas besoin de r√©entra√Æner
- Tra√ßabilit√© des sources
- Faible co√ªt
- Informations toujours √† jour

‚ùå **Limites**
- D√©pend de la qualit√© de la recherche vectorielle
- Contexte limit√© par la taille du prompt
- Ne "comprend" pas vraiment le domaine

### Fine-Tuning
‚úÖ **Avantages**
- Mod√®le adapt√© au domaine
- Meilleure compr√©hension du vocabulaire m√©tier
- Style de r√©ponse coh√©rent
- Pas besoin de contexte externe

‚ùå **Limites**
- Co√ªteux (temps + argent)
- N√©cessite beaucoup de donn√©es d'entra√Ænement
- Difficile √† mettre √† jour (r√©entra√Ænement)
- Peut "oublier" des informations g√©n√©rales

### Approche Hybride (RECOMMAND√âE)
üéØ **Fine-tuning** pour :
- Comprendre le vocabulaire m√©tier (ex: GIDAF, MYGUSI, etc.)
- Style de r√©ponse adapt√© (ton professionnel SI)
- Structure de r√©ponse coh√©rente
- Compr√©hension des relations entre applications

üéØ **RAG** pour :
- Informations factuelles sp√©cifiques (h√©bergeur, technologies)
- Donn√©es qui changent fr√©quemment
- R√©f√©rences pr√©cises (IDs d'applications)
- Nouvelles applications ajout√©es

## 2. Architecture Propos√©e

### 2.1 Structure du projet r√©organis√©e

```
dyag/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ dyag/
‚îÇ       ‚îú‚îÄ‚îÄ commands/          # Commandes CLI existantes
‚îÇ       ‚îú‚îÄ‚îÄ rag/              # Module RAG (NOUVEAU)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ query.py      # RAGQuerySystem (ancien rag_query.py)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ indexer.py    # Indexation des chunks
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ embeddings.py # Gestion des embeddings
‚îÇ       ‚îú‚îÄ‚îÄ finetuning/       # Module Fine-tuning (NOUVEAU)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py    # Pr√©paration datasets
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py    # Entra√Ænement
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py  # √âvaluation du mod√®le
‚îÇ       ‚îú‚îÄ‚îÄ llm/              # Module LLM (R√âORGANIS√â)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ providers.py  # LLM providers (ancien llm_providers.py)
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ models.py     # Gestion des mod√®les fine-tun√©s
‚îÇ       ‚îî‚îÄ‚îÄ hybrid/           # Module Hybride (NOUVEAU)
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îî‚îÄ‚îÄ query.py      # Syst√®me hybride RAG + Fine-tuning
‚îú‚îÄ‚îÄ formation/                # Scripts de formation
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py
‚îÇ   ‚îú‚îÄ‚îÄ multi_format_reader.py
‚îÇ   ‚îî‚îÄ‚îÄ finetuning_examples/  # Exemples de fine-tuning (NOUVEAU)
‚îú‚îÄ‚îÄ scripts/                  # Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îú‚îÄ‚îÄ prepare_finetuning_data.py  # Pr√©parer donn√©es (NOUVEAU)
‚îÇ   ‚îî‚îÄ‚îÄ finetune_model.py          # Lancer fine-tuning (NOUVEAU)
‚îú‚îÄ‚îÄ data/                     # Donn√©es (NOUVEAU)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # Donn√©es brutes
‚îÇ   ‚îú‚îÄ‚îÄ processed/           # Donn√©es trait√©es
‚îÇ   ‚îî‚îÄ‚îÄ finetuning/         # Datasets de fine-tuning
‚îÇ       ‚îú‚îÄ‚îÄ train.jsonl
‚îÇ       ‚îú‚îÄ‚îÄ validation.jsonl
‚îÇ       ‚îî‚îÄ‚îÄ test.jsonl
‚îú‚îÄ‚îÄ models/                   # Mod√®les fine-tun√©s (NOUVEAU)
‚îÇ   ‚îî‚îÄ‚îÄ dyag-gpt-4o-mini-v1/
‚îî‚îÄ‚îÄ chroma_db/               # Base vectorielle (existant)
```

### 2.2 Workflow Hybride

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  User Question  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Hybrid Query System     ‚îÇ
‚îÇ (RAG + Fine-tuned LLM)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
    v         v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAG  ‚îÇ  ‚îÇ Fine-tuned   ‚îÇ
‚îÇ       ‚îÇ  ‚îÇ LLM Provider ‚îÇ
‚îÇ 1. Search chunks       ‚îÇ
‚îÇ 2. Retrieve context    ‚îÇ
‚îÇ                        ‚îÇ
‚îÇ 3. Send to fine-tuned  ‚îÇ‚îÄ‚îÄ>‚îÇ
‚îÇ    model with context  ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                             ‚îÇ
                             v
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ   Response   ‚îÇ
                      ‚îÇ  (Enhanced)  ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.3 Impl√©mentation du syst√®me hybride

```python
# src/dyag/hybrid/query.py
from typing import Dict, Optional
from ..rag.query import RAGQuerySystem
from ..llm.providers import LLMProviderFactory

class HybridQuerySystem:
    """
    Syst√®me hybride combinant RAG et mod√®le fine-tun√©.

    Le mod√®le fine-tun√© comprend le domaine m√©tier.
    Le RAG fournit les informations factuelles sp√©cifiques.
    """

    def __init__(
        self,
        use_finetuned: bool = True,
        finetuned_model_id: Optional[str] = None,
        **rag_kwargs
    ):
        # Initialiser le RAG
        self.rag = RAGQuerySystem(**rag_kwargs)

        # Si mod√®le fine-tun√© disponible, l'utiliser
        if use_finetuned and finetuned_model_id:
            # Remplacer le provider LLM par le mod√®le fine-tun√©
            self.rag.llm_provider = LLMProviderFactory.create_provider(
                provider='openai',  # OpenAI supporte les mod√®les fine-tun√©s
                model=finetuned_model_id  # ex: "ft:gpt-4o-mini-2024-07-18:org::id"
            )

    def ask(self, question: str, **kwargs) -> Dict:
        """
        Pose une question avec le syst√®me hybride.

        Le mod√®le fine-tun√© re√ßoit le contexte RAG et g√©n√®re
        une r√©ponse adapt√©e au domaine m√©tier.
        """
        # D√©l√©guer au RAG qui utilisera automatiquement
        # le mod√®le fine-tun√© si configur√©
        return self.rag.ask(question, **kwargs)
```

## 3. Plan de mise en ≈ìuvre

### Phase 1 : Pr√©paration des donn√©es de fine-tuning (1-2 jours)

#### 3.1 Cr√©er un dataset de qualit√©

Le fine-tuning n√©cessite des exemples de conversations au format :
```jsonl
{"messages": [
  {"role": "system", "content": "Tu es un assistant sp√©cialis√©..."},
  {"role": "user", "content": "Qui h√©berge GIDAF ?"},
  {"role": "assistant", "content": "GIDAF est h√©berg√© par..."}
]}
```

**Sources de donn√©es** :
1. G√©n√©rer des Q&A synth√©tiques depuis vos chunks existants
2. Logs de questions r√©elles (si disponible)
3. Exemples manuels de bonnes r√©ponses

#### 3.2 Script de pr√©paration

```python
# scripts/prepare_finetuning_data.py
"""
G√©n√®re un dataset de fine-tuning depuis les chunks RAG.

Strat√©gie:
1. Pour chaque application, g√©n√©rer 5-10 questions types
2. Utiliser un LLM (GPT-4) pour g√©n√©rer les r√©ponses
3. Valider la qualit√©
4. Splitter en train/validation/test (80/10/10)
"""
```

**Quantit√© recommand√©e** : 500-2000 exemples
- Minimum : 100 exemples (pour commencer)
- Optimal : 1000+ exemples (meilleure qualit√©)

### Phase 2 : Fine-tuning du mod√®le (2-4 heures selon provider)

#### 3.1 Providers supportant le fine-tuning

| Provider | Mod√®les | Co√ªt | Temps |
|----------|---------|------|-------|
| **OpenAI** | gpt-4o-mini (recommand√©)<br>gpt-4o<br>gpt-3.5-turbo | ~$0.80/1M tokens training<br>~$2.40/1M tokens usage | 10min-2h |
| **Anthropic** | ‚ùå Pas de fine-tuning public | - | - |
| **Ollama** | ‚úÖ Fine-tuning local possible<br>(LLaMA, Mistral) | Gratuit | 2-12h |

**Recommandation** : Commencer avec **gpt-4o-mini** d'OpenAI
- Excellent rapport qualit√©/prix
- Rapide √† entra√Æner
- API simple

#### 3.2 Script de fine-tuning

```python
# scripts/finetune_model.py
"""
Lance le fine-tuning sur OpenAI ou Ollama.

Usage:
    python scripts/finetune_model.py --provider openai --model gpt-4o-mini
    python scripts/finetune_model.py --provider ollama --model llama3.2
"""
```

### Phase 3 : Int√©gration hybride (1 jour)

1. **Cr√©er le module hybride** : `src/dyag/hybrid/query.py`
2. **Adapter le chat** : Modifier `scripts/chat.py` pour utiliser le syst√®me hybride
3. **Tester** : Comparer RAG seul vs Hybride

#### 3.3 Chat hybride

```python
# scripts/chat.py (modifi√©)
from dyag.hybrid.query import HybridQuerySystem

# Mode hybride (RAG + fine-tuned)
rag = HybridQuerySystem(
    use_finetuned=True,
    finetuned_model_id="ft:gpt-4o-mini-2024-07-18:org::id"
)

# OU mode RAG classique
rag = HybridQuerySystem(use_finetuned=False)
```

### Phase 4 : √âvaluation et it√©ration (continu)

#### M√©triques √† suivre :
- **Pr√©cision** : Les r√©ponses sont-elles factuellement correctes ?
- **Pertinence** : Les sources cit√©es sont-elles pertinentes ?
- **Style** : Le ton est-il adapt√© (professionnel SI) ?
- **Compl√©tude** : Les r√©ponses sont-elles compl√®tes ?

## 4. Estimation des co√ªts

### Fine-tuning OpenAI gpt-4o-mini

**Exemple avec 1000 exemples** :
- Dataset : ~500K tokens
- Training : $0.40 (500K tokens √ó $0.80/1M)
- **Usage** : $2.40/1M tokens (1.5√ó le prix de gpt-4o-mini standard)

**ROI** :
- Meilleure qualit√© de r√©ponse
- Moins de contexte RAG n√©cessaire ‚Üí moins de tokens
- Style coh√©rent ‚Üí moins de prompt engineering

### Alternative gratuite : Ollama fine-tuning

- **Co√ªt** : 0‚Ç¨ (local)
- **Temps** : 2-12h selon GPU
- **Qualit√©** : Variable selon mod√®le de base

## 5. R√©organisation n√©cessaire ?

### Option 1 : R√©organisation minimale (RECOMMAND√âE pour d√©marrer)

**Changements** :
1. Garder l'architecture actuelle
2. Ajouter juste :
   - `scripts/prepare_finetuning_data.py`
   - `scripts/finetune_model.py`
   - `data/finetuning/` directory
3. Modifier `RAGQuerySystem` pour accepter un `finetuned_model_id`

**Avantages** :
- Changements minimaux
- Pas de refactoring
- D√©marrage rapide

### Option 2 : R√©organisation compl√®te

**Changements** :
1. Restructurer selon l'architecture propos√©e (section 2.1)
2. S√©parer RAG / Fine-tuning / Hybrid en modules
3. Meilleure s√©paration des responsabilit√©s

**Avantages** :
- Architecture plus propre
- Scalabilit√©
- Maintenance facilit√©e

**Recommandation** : Commencer avec **Option 1**, puis migrer vers **Option 2** si le fine-tuning prouve sa valeur.

## 6. Exemple concret : Cas d'usage

### Sc√©nario : "Qui h√©berge GIDAF ?"

#### Avec RAG seul :
```
User: Qui h√©berge GIDAF ?

[RAG] ‚Üí Recherche chunks ‚Üí Trouve "GIDAF est h√©berg√© par SI-RAPP"
[GPT-4o-mini standard] + contexte ‚Üí R√©pond

R√©ponse: "Selon les informations, GIDAF est h√©berg√© par SI-RAPP."
```

**Probl√®me** : R√©ponse g√©n√©rique, pas de contexte m√©tier

#### Avec RAG + Fine-tuning :
```
User: Qui h√©berge GIDAF ?

[RAG] ‚Üí Recherche chunks ‚Üí Trouve "GIDAF est h√©berg√© par SI-RAPP"
[GPT-4o-mini FINE-TUN√â] + contexte ‚Üí R√©pond avec compr√©hension m√©tier

R√©ponse: "GIDAF (Gestion Int√©gr√©e des Applications et Flux) est h√©berg√©
par SI-RAPP, la plateforme d'h√©bergement interne d√©di√©e aux applications
critiques du SI. Cette application b√©n√©ficie d'un SLA √©lev√© et d'une
supervision 24/7. [Source: chunk_383]"
```

**Gain** :
- Compr√©hension du vocabulaire (SI-RAPP, SLA)
- Contexte m√©tier enrichi
- Style professionnel coh√©rent
- Informations compl√©mentaires pertinentes

## 7. Prochaines √©tapes recommand√©es

### √âtape 1 : Valider l'approche (1 jour)
1. ‚úÖ Lire cette architecture
2. ‚úÖ D√©cider : Option 1 (minimal) ou Option 2 (complet)
3. ‚úÖ Valider le budget (fine-tuning OpenAI ~$5-20)

### √âtape 2 : Pr√©paration (2 jours)
1. Cr√©er `scripts/prepare_finetuning_data.py`
2. G√©n√©rer 100-200 exemples de Q&A
3. Valider la qualit√© manuellement

### √âtape 3 : Premier fine-tuning (1 jour)
1. Cr√©er `scripts/finetune_model.py`
2. Lancer le fine-tuning sur gpt-4o-mini
3. R√©cup√©rer le model ID

### √âtape 4 : Int√©gration (1 jour)
1. Modifier `RAGQuerySystem` pour accepter le model ID
2. Tester avec `scripts/chat.py`
3. Comparer qualit√© RAG vs Hybride

### √âtape 5 : It√©ration (continu)
1. Collecter les questions r√©elles
2. Enrichir le dataset
3. Re-fine-tuner p√©riodiquement

## 8. FAQ

### Q: Faut-il ABSOLUMENT fine-tuner ?
**R:** Non. RAG seul fonctionne bien. Fine-tuning am√©liore la qualit√© mais n'est pas obligatoire.

### Q: Quel est le meilleur provider pour fine-tuning ?
**R:**
- **OpenAI gpt-4o-mini** : Meilleur rapport qualit√©/prix/simplicit√©
- **Ollama** : Gratuit mais plus complexe

### Q: Combien d'exemples faut-il ?
**R:** Minimum 100, optimal 500-1000+

### Q: Peut-on fine-tuner Claude/Anthropic ?
**R:** Non, pas de fine-tuning public. Utiliser OpenAI ou Ollama.

### Q: Le fine-tuning remplace-t-il le RAG ?
**R:** **NON**. Ils sont compl√©mentaires :
- Fine-tuning = comprendre le domaine
- RAG = faits sp√©cifiques et √† jour

## Conclusion

‚úÖ **OUI**, faites cohabiter RAG et fine-tuning
‚úÖ **NON**, pas besoin de r√©organisation majeure pour d√©marrer
‚úÖ Commencer avec l'**Option 1** (minimal) et it√©rer
‚úÖ Provider recommand√© : **OpenAI gpt-4o-mini**
‚úÖ Budget : ~$5-20 pour le premier fine-tuning

**Le meilleur syst√®me = RAG (faits) + Fine-tuning (compr√©hension m√©tier)**
