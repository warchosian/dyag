# Guide de DÃ©marrage Rapide - SystÃ¨me RAG

Guide pour mettre en place et utiliser le systÃ¨me de Questions & RÃ©ponses avec RAG en **5 minutes**.

## PrÃ©requis

1. **Python 3.8+** installÃ©
2. **ClÃ© API OpenAI** (pour les rÃ©ponses du LLM)
   - Obtenez une clÃ© sur: https://platform.openai.com/api-keys
   - CoÃ»t estimÃ©: ~$0.01 par question (avec GPT-4o-mini)

## Installation - 3 Ã©tapes

### Ã‰tape 1: Installer les dÃ©pendances

```bash
pip install -r requirements-rag.txt
```

Ceci installe:
- `chromadb` - Base vectorielle
- `sentence-transformers` - Embeddings (gratuit, local)
- `openai` - Client LLM
- `langchain` - Framework RAG
- `streamlit` - Interface web (optionnel)

**Temps:** ~2 minutes

### Ã‰tape 2: Configurer la clÃ© API OpenAI

CrÃ©ez un fichier `.env` Ã  la racine du projet:

```env
OPENAI_API_KEY=sk-proj-...votre_cle_ici...
```

**Temps:** 30 secondes

### Ã‰tape 3: Indexer vos chunks

```bash
# Depuis la racine du projet
python scripts/index_chunks.py applications_rag_optimal.jsonl
```

Sortie attendue:
```
Connexion a ChromaDB: .\chroma_db
Chargement du modele d'embedding: all-MiniLM-L6-v2
Modele charge avec dimension: 384

Chargement des chunks depuis: applications_rag_optimal.jsonl
Chunks charges: 1628

Indexation de 1628 chunks...
Lot 1/17: 100 chunks indexes
Lot 2/17: 100 chunks indexes
...
Lot 17/17: 28 chunks indexes

Indexation terminee:
  - Indexes: 1628
  - Erreurs: 0
  - Taux de reussite: 100.0%
```

**Temps:** ~2 minutes (selon votre machine)

## Utilisation

### Option A: Mode interactif simple

```bash
python -m dyag.rag_query
```

```
Initialisation du systeme RAG...

Statistiques:
  - Chunks indexes: 1628
  - Modele LLM: gpt-4o-mini

==============================================================
Mode interactif - Posez vos questions (Ctrl+C pour quitter)
==============================================================

â“ Question: Qui hÃ©berge GIDAF ?

ğŸ” Recherche en cours...

ğŸ’¬ RÃ©ponse:
GIDAF est hÃ©bergÃ© par le BRGM (Bureau de Recherches GÃ©ologiques et MiniÃ¨res).
Source: [Chunk a1b2c3d4]

ğŸ“Š MÃ©tadonnÃ©es:
  - Sources: 5 chunks
  - Tokens: 342
  - IDs: a1b2c3d4, e5f6g7h8, i9j0k1l2...
```

### Option B: Script Python

```python
from dyag.rag_query import RAGQuerySystem

# Initialiser
rag = RAGQuerySystem()

# Poser une question
result = rag.ask("Quelles sont les applications en Java ?")

# Afficher la rÃ©ponse
print(result['answer'])

# AccÃ©der aux mÃ©tadonnÃ©es
print(f"Sources: {len(result['sources'])} chunks")
print(f"Tokens utilisÃ©s: {result['tokens_used']}")
print(f"Chunks: {result['sources']}")
```

### Option C: Script complet avec guide

```bash
python scripts/example_rag_complete.py
```

Ce script vous guide Ã  travers:
1. CrÃ©ation des chunks (si nÃ©cessaire)
2. Indexation
3. Mode Q&A interactif

## Exemples de questions

### Questions simples

```
Qui hÃ©berge GIDAF ?
Quelle est la description de MYGUSI ?
Quel est le gestionnaire de WIKISI ?
```

### Questions complexes

```
Quelles sont toutes les applications hÃ©bergÃ©es par le BRGM ?
Liste les applications utilisant Java comme technologie.
Quels sont les services web disponibles pour les applications de gestion ?
```

### Questions analytiques

```
Combien d'applications utilisent Oracle comme base de donnÃ©es ?
Quelles sont les principales technologies utilisÃ©es dans le SI ?
Qui sont les principaux hÃ©bergeurs d'applications ?
```

## Filtrage par application

Pour poser une question sur une **application spÃ©cifique**:

```python
# Uniquement sur l'application ID 383 (GIDAF)
result = rag.ask(
    "Quelle est la description ?",
    filter_metadata={"source_id": "383"}
)
```

## Personnalisation

### Changer le nombre de chunks

```python
# Utiliser 10 chunks au lieu de 5 (par dÃ©faut)
result = rag.ask(
    "Ma question",
    n_chunks=10  # Plus de contexte = rÃ©ponses plus complÃ¨tes
)
```

### Ajuster la crÃ©ativitÃ©

```python
# Temperature = 0 (prÃ©cis) Ã  1 (crÃ©atif)
result = rag.ask(
    "Ma question",
    temperature=0.0  # RÃ©ponses trÃ¨s factuelles
)
```

### Utiliser un autre modÃ¨le LLM

```python
rag = RAGQuerySystem(
    llm_model="gpt-4o"  # Plus prÃ©cis mais plus cher
)
```

## Interface Web (Optionnel)

CrÃ©ez `app_streamlit.py`:

```python
import streamlit as st
from dyag.rag_query import RAGQuerySystem

st.title("Recherche d'Applications - RAG")

# Initialiser (avec cache)
@st.cache_resource
def load_rag():
    return RAGQuerySystem()

rag = load_rag()

# Interface
question = st.text_input("Posez votre question:")

if question:
    with st.spinner("Recherche en cours..."):
        result = rag.ask(question)

    st.subheader("RÃ©ponse:")
    st.write(result['answer'])

    st.subheader("Sources:")
    st.write(f"{len(result['sources'])} chunks utilisÃ©s")
    st.write(f"Tokens: {result['tokens_used']}")

    with st.expander("Voir les chunks sources"):
        for i, chunk in enumerate(result['chunks_used'], 1):
            st.write(f"**Chunk {i}** (ID: {chunk['id']})")
            st.write(chunk['content'][:300] + "...")
```

Lancez avec:
```bash
streamlit run app_streamlit.py
```

## CoÃ»ts EstimÃ©s

Avec **GPT-4o-mini** (modÃ¨le par dÃ©faut):

| Usage | Questions/mois | CoÃ»t/mois |
|-------|----------------|-----------|
| LÃ©ger | 100 | $1-2 |
| Moyen | 500 | $5-10 |
| Intensif | 2000 | $20-40 |

**Gratuit (local):**
- Remplacez OpenAI par **LLaMA 3.1 8B** (voir `rag-modules-guide.md`)
- NÃ©cessite ~10 GB RAM et GPU recommandÃ©

## RÃ©solution de ProblÃ¨mes

### Erreur: "Collection 'applications' non trouvÃ©e"

```bash
# RÃ©indexer les chunks
python scripts/index_chunks.py applications_rag_optimal.jsonl --reset
```

### Erreur: "ClÃ© API OpenAI requise"

```bash
# VÃ©rifier le fichier .env
cat .env

# Ou dÃ©finir la variable d'environnement
export OPENAI_API_KEY=sk-proj-...
```

### Erreur: "No module named 'chromadb'"

```bash
# RÃ©installer les dÃ©pendances
pip install -r requirements-rag.txt
```

### RÃ©ponses non pertinentes

1. **Augmenter le nombre de chunks:**
   ```python
   result = rag.ask("Question", n_chunks=10)
   ```

2. **Baisser la tempÃ©rature:**
   ```python
   result = rag.ask("Question", temperature=0.0)
   ```

3. **Utiliser un meilleur modÃ¨le:**
   ```python
   rag = RAGQuerySystem(llm_model="gpt-4o")
   ```

## Commandes Utiles

```bash
# RÃ©indexer complÃ¨tement
python scripts/index_chunks.py applications_rag_optimal.jsonl --reset

# Indexer avec un autre modÃ¨le d'embedding
python scripts/index_chunks.py applications_rag_optimal.jsonl \
    --embedding-model all-mpnet-base-v2

# Mode Q&A avec statistiques
python -m dyag.rag_query

# Script complet guidÃ©
python scripts/example_rag_complete.py
```

## Architecture SimplifiÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question   â”‚
â”‚  utilisateurâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Embedding    â”‚â”€â”€â”€â”€â”€>â”‚ ChromaDB    â”‚
â”‚  (local, gratuit)â”‚      â”‚ 1628 chunks â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ Top 5 chunks
                                 â”‚ similaires
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ 2. Contexte â”‚
                          â”‚  (5 chunks) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ 3. OpenAI   â”‚
                          â”‚  GPT-4o-miniâ”‚
                          â”‚  (payant)   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  RÃ©ponse    â”‚
                          â”‚  + Sources  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Prochaines Ã‰tapes

1. **Testez avec vos propres questions** pour valider la pertinence
2. **Ajustez les paramÃ¨tres** (n_chunks, temperature) selon vos besoins
3. **Consultez la documentation avancÃ©e** dans `rag-modules-guide.md`
4. **Explorez l'architecture hybride** dans `chunks-for-management.md`

## Support

- **Documentation complÃ¨te:** `doc/rag-modules-guide.md`
- **Algorithme de chunking:** `doc/rag-chunks-algo.md`
- **Use cases:** `doc/chunks-why.md`
- **Dashboard management:** `doc/chunks-for-management.md`
