"""
RAG Evaluation Report Generator

Generates detailed analysis reports from RAG evaluation results.
"""

import json
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from difflib import SequenceMatcher


def calculate_similarity(expected: str, obtained: str) -> float:
    """
    Calculate similarity ratio between expected and obtained answers.

    Args:
        expected: Expected answer
        obtained: Obtained answer

    Returns:
        Similarity ratio (0.0 to 1.0)
    """
    if not expected or not obtained:
        return 0.0

    # Normalize strings
    exp_lower = expected.lower().strip()
    obt_lower = obtained.lower().strip()

    # Calculate sequence similarity
    similarity = SequenceMatcher(None, exp_lower, obt_lower).ratio()

    # Bonus if expected is contained in obtained
    if exp_lower in obt_lower:
        similarity = max(similarity, 0.7)

    return similarity


def categorize_answer_quality(similarity: float, obtained: str) -> tuple[str, str]:
    """
    Categorize answer quality based on similarity and content.

    Returns:
        (status_emoji, status_text)
    """
    # Check for common failure patterns
    if any(phrase in obtained.lower() for phrase in [
        "je n'ai pas trouv√©",
        "je n'ai pas d'information",
        "je ne trouve aucune",
        "je ne peux pas",
        "d√©sol√©",
        "aucune information"
    ]):
        return "‚ùå", "Aucune information trouv√©e"

    if similarity >= 0.8:
        return "‚úÖ", "R√©ponse correcte"
    elif similarity >= 0.5:
        return "‚ö†Ô∏è", "R√©ponse partielle"
    elif similarity >= 0.2:
        return "‚ùå", "R√©ponse incorrecte"
    else:
        return "‚ùå", "R√©ponse totalement erron√©e"


def analyze_chunks(results: List[Dict]) -> Dict[str, int]:
    """
    Analyze chunk distribution across questions.

    Returns:
        Dictionary of chunk_id -> frequency
    """
    chunk_freq = {}
    for result in results:
        for chunk_id in result.get("sources", []):
            chunk_freq[chunk_id] = chunk_freq.get(chunk_id, 0) + 1

    # Sort by frequency
    return dict(sorted(chunk_freq.items(), key=lambda x: x[1], reverse=True))


def generate_markdown_report(
    results_data: Dict[str, Any],
    output_path: str = None,
    verbose: bool = False
) -> str:
    """
    Generate a detailed markdown evaluation report.

    Args:
        results_data: Evaluation results from JSON
        output_path: Optional output file path
        verbose: Show detailed progress

    Returns:
        Generated markdown content
    """
    metadata = results_data.get("metadata", {})
    results = results_data.get("results", [])

    if not results:
        raise ValueError("No results found in data")

    # Calculate similarities and stats
    similarities = []
    correct_count = 0
    partial_count = 0
    incorrect_count = 0

    for result in results:
        sim = calculate_similarity(
            result.get("expected", ""),
            result.get("answer", "")
        )
        similarities.append(sim)

        if sim >= 0.8:
            correct_count += 1
        elif sim >= 0.5:
            partial_count += 1
        else:
            incorrect_count += 1

    avg_similarity = sum(similarities) / len(similarities) if similarities else 0

    # Analyze chunks
    chunk_freq = analyze_chunks(results)

    # Build report
    report = []

    # Header
    report.append("# Rapport d'√âvaluation RAG\n")
    report.append(f"**Date de g√©n√©ration**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report.append(f"**Date d'√©valuation**: {metadata.get('timestamp', 'N/A')}\n")
    report.append(f"**Mod√®le LLM**: {metadata.get('model', 'N/A')}\n")
    report.append(f"**Chunks par question**: {metadata.get('n_chunks', 'N/A')}\n")
    report.append("\n---\n\n")

    # Executive Summary
    report.append("## üìä R√©sum√© Ex√©cutif\n\n")
    report.append("### M√©triques Techniques\n\n")
    report.append("| M√©trique | Valeur |\n")
    report.append("|----------|--------|\n")
    report.append(f"| **Questions test√©es** | {metadata.get('total_questions', 0)} |\n")
    report.append(f"| **Succ√®s technique** | {metadata.get('successful', 0)}/{metadata.get('total_questions', 0)} ({metadata.get('successful', 0)/metadata.get('total_questions', 1)*100:.1f}%) |\n")
    report.append(f"| **Temps moyen** | {metadata.get('total_time', 0)/metadata.get('total_questions', 1):.1f}s |\n")
    report.append(f"| **Tokens moyens** | {metadata.get('total_tokens', 0)//metadata.get('total_questions', 1)} tokens |\n")
    report.append(f"| **Temps total** | {metadata.get('total_time', 0)/60:.1f} minutes |\n\n")

    report.append("### M√©triques Qualitatives\n\n")
    report.append("| M√©trique | Valeur | Commentaire |\n")
    report.append("|----------|--------|-------------|\n")
    report.append(f"| **Score moyen de similarit√©** | **{avg_similarity*100:.1f}%** | Correspondance attendu/obtenu |\n")
    report.append(f"| **R√©ponses correctes** | {correct_count}/{len(results)} ({correct_count/len(results)*100:.1f}%) | Similarit√© ‚â• 80% |\n")
    report.append(f"| **R√©ponses partielles** | {partial_count}/{len(results)} ({partial_count/len(results)*100:.1f}%) | Similarit√© 50-80% |\n")
    report.append(f"| **R√©ponses incorrectes** | {incorrect_count}/{len(results)} ({incorrect_count/len(results)*100:.1f}%) | Similarit√© < 50% |\n\n")

    # Detailed Analysis
    report.append("---\n\n")
    report.append("## üîç Analyse D√©taill√©e par Question\n\n")

    for i, result in enumerate(results, 1):
        question = result.get("question", "N/A")
        expected = result.get("expected", "")
        obtained = result.get("answer", "")
        sources = result.get("sources", [])
        tokens = result.get("tokens", 0)
        time = result.get("time", 0)

        sim = calculate_similarity(expected, obtained)
        emoji, status = categorize_answer_quality(sim, obtained)

        report.append(f"### Question {i}: {question}\n\n")
        report.append("| M√©trique | Valeur |\n")
        report.append("|----------|--------|\n")
        report.append(f"| **Statut** | {emoji} {status} |\n")
        report.append(f"| **Similarit√©** | {sim*100:.1f}% |\n")
        report.append(f"| **Temps** | {time:.1f}s |\n")
        report.append(f"| **Tokens** | {tokens} |\n")
        report.append(f"| **Chunks** | {', '.join(sources[:5])} |\n\n")

        # Expected vs Obtained
        report.append("**R√©ponse attendue:**\n")
        exp_preview = expected[:200] + "..." if len(expected) > 200 else expected
        report.append(f"> {exp_preview}\n\n")

        report.append("**R√©ponse obtenue:**\n")
        obt_preview = obtained[:200] + "..." if len(obtained) > 200 else obtained
        report.append(f"> {obt_preview}\n\n")

        # Analysis
        if sim < 0.5:
            report.append("**Analyse:**\n")
            if "je n'ai pas trouv√©" in obtained.lower() or "je n'ai pas d'information" in obtained.lower():
                report.append("- ‚ùå Le syst√®me n'a trouv√© aucune information pertinente\n")
            elif sim < 0.1:
                report.append("- ‚ùå R√©ponse totalement hors sujet ou hallucination\n")
            else:
                report.append("- ‚ö†Ô∏è R√©ponse incorrecte mais contient des √©l√©ments pertinents\n")

        report.append("\n---\n\n")

    # Chunk Analysis
    report.append("## üì¶ Analyse des Chunks Retourn√©s\n\n")
    report.append("### Chunks les Plus Fr√©quents\n\n")
    report.append("| Chunk ID | Fr√©quence | Pourcentage |\n")
    report.append("|----------|-----------|-------------|\n")

    total_questions = len(results)
    for chunk_id, freq in list(chunk_freq.items())[:10]:
        percentage = freq / total_questions * 100
        report.append(f"| {chunk_id} | {freq}/{total_questions} | {percentage:.1f}% |\n")

    report.append("\n")

    # Recommendations
    report.append("---\n\n")
    report.append("## üí° Recommandations\n\n")

    if avg_similarity < 0.3:
        report.append("### üî¥ Critique - Action Imm√©diate Requise\n\n")
        report.append("Le score de similarit√© moyen est tr√®s faible ({:.1f}%). Les probl√®mes majeurs identifi√©s:\n\n".format(avg_similarity * 100))
        report.append("1. **Revoir la strat√©gie de chunking**\n")
        report.append("   - Utiliser `size-based` au lieu de `markdown-headers`\n")
        report.append("   - Augmenter la taille des chunks (1500-2000 tokens)\n")
        report.append("   - Ajouter overlap (300-500 tokens)\n\n")
        report.append("2. **Enrichir les m√©tadonn√©es**\n")
        report.append("   - Ajouter identifiants d'entit√©s dans chaque chunk\n")
        report.append("   - Tagger les chunks par cat√©gorie\n\n")
        report.append("3. **Tester d'autres mod√®les d'embedding**\n")
        report.append("   - `all-mpnet-base-v2` (meilleure qualit√©)\n")
        report.append("   - `multilingual-e5-large` (multilingue)\n\n")
    elif avg_similarity < 0.6:
        report.append("### üü° Moyen - Am√©liorations Recommand√©es\n\n")
        report.append("Le score de similarit√© moyen est moyen ({:.1f}%). Am√©liorations sugg√©r√©es:\n\n".format(avg_similarity * 100))
        report.append("1. **Optimiser le prompt syst√®me**\n")
        report.append("   - R√©duire la verbosit√©\n")
        report.append("   - Am√©liorer les instructions de formatage\n\n")
        report.append("2. **Augmenter le nombre de chunks r√©cup√©r√©s**\n")
        report.append("   - Tester avec n_chunks=10 au lieu de 5\n\n")
        report.append("3. **Impl√©menter le reranking**\n")
        report.append("   - Reclasser les chunks apr√®s r√©cup√©ration\n\n")
    else:
        report.append("### üü¢ Bon - Optimisations Mineures\n\n")
        report.append("Le score de similarit√© moyen est bon ({:.1f}%). Optimisations possibles:\n\n".format(avg_similarity * 100))
        report.append("1. **Fine-tuner le mod√®le LLM**\n")
        report.append("   - Utiliser les donn√©es d'√©valuation pour le fine-tuning\n\n")
        report.append("2. **Monitorer les performances**\n")
        report.append("   - Mettre en place des alertes sur les m√©triques\n\n")

    # Footer
    report.append("\n---\n\n")
    report.append("**Rapport g√©n√©r√© automatiquement par DYAG**\n")
    report.append(f"**Outil**: `dyag generate-evaluation-report`\n")
    report.append(f"**Version**: 0.8.0+\n")

    markdown_content = "".join(report)

    # Save if output path provided
    if output_path:
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        if verbose:
            print(f"[OK] Rapport g√©n√©r√©: {output_path}")

    return markdown_content


def generate_report_from_file(
    results_file: str,
    output_path: str = None,
    verbose: bool = False
) -> str:
    """
    Generate report from evaluation results JSON file.

    Args:
        results_file: Path to evaluation results JSON
        output_path: Optional output markdown file
        verbose: Show progress

    Returns:
        Generated markdown content
    """
    if verbose:
        print(f"Chargement des r√©sultats: {results_file}")

    with open(results_file, 'r', encoding='utf-8') as f:
        results_data = json.load(f)

    if verbose:
        total = results_data.get("metadata", {}).get("total_questions", 0)
        print(f"[OK] {total} questions charg√©es\n")

    return generate_markdown_report(results_data, output_path, verbose)
