# Rapport Final - Am√©liorations RAG Phase 1

**Date** : 2025-12-24
**Auteur** : Claude Sonnet 4.5
**Contexte** : Am√©lioration du syst√®me RAG sur 10 applications

---

## Vue d'ensemble

Ce rapport synth√©tise les am√©liorations apport√©es au syst√®me RAG (Retrieval-Augmented Generation) pour r√©soudre les probl√®mes de faible similarit√© et d'hallucinations identifi√©s dans les √©valuations initiales.

## Modifications Impl√©ment√©es (Phase 1)

### 1. M√©tadonn√©es Normalis√©es

**Fichier modifi√©** : `src/dyag/commands/create_rag.py`

**Changements** :
- Ajout de cl√©s normalis√©es dans les m√©tadonn√©es de chaque chunk :
  - `app_id` : ID de l'application (normalis√©)
  - `app_name` : Nom de l'application (normalis√©)
  - `app_state` : √âtat de l'application (normalis√©)
- Support des cl√©s JSON avec casse variable ('id' vs 'Id', 'nom' vs 'Nom')
- Conversion des listes/dicts en strings pour compatibilit√© ChromaDB

### 2. Mod√®le d'Embedding Am√©lior√©

**Changement** : `all-MiniLM-L6-v2` ‚Üí `BAAI/bge-small-en-v1.5`

**Avantages observ√©s** :
- Meilleure qualit√© d'embedding pour le contenu technique
- Am√©lioration de la distance vectorielle (1.03 ‚Üí 0.57 sur tests manuels)
- Les bons chunks sont maintenant retrouv√©s

### 3. Prompt Syst√®me Strict

**Fichier modifi√©** : `src/dyag/rag_query.py`

**Nouveau prompt** :
```
Tu es un assistant expert en gestion applicative. R√©ponds uniquement avec les faits pr√©sents dans les extraits fournis.
- Sois bref (max 2 phrases).
- Si l'information n'est pas dans les extraits, r√©ponds : "Non disponible".
- N'invente jamais de dates, noms ou √©tats.
- Ne fais aucune d√©duction ou inf√©rence au-del√† du contexte fourni.
```

**Note** : Le LLM phi3 ne respecte pas ce prompt strict (voir probl√®mes ci-dessous).

### 4. Corrections de Bugs

- ‚úÖ Fix case-sensitivity des cl√©s JSON
- ‚úÖ Fix metadata ChromaDB (conversion listes/dicts ‚Üí strings)
- ‚úÖ Fix extraction contenu chunks overview
- ‚úÖ Fix embedding model dans evaluate-rag command (ajout param√®tre --embedding-model)
- ‚úÖ Support dual format dans load_dataset (messages + direct question/answer)

### 5. Nouvelle Commande CLI

**Ajout** : `python -m dyag compare-rag baseline.json improved.json`

**Impl√©mentation** :
- Module logique : `src/dyag/rag/comparison.py`
- Wrapper CLI : `src/dyag/commands/compare_rag.py`

---

## R√©sultats de l'√âvaluation

### Configuration d'√âvaluation

```bash
python -m dyag evaluate-rag evaluation/questions_10apps_rag.jsonl \
    --chroma-path ./chroma_db_10apps_v3 \
    --collection applications_v3 \
    --embedding-model BAAI/bge-small-en-v1.5 \
    --n-chunks 5 \
    --max-questions 20 \
    --output evaluation/results_phase1_10apps.json
```

- **Dataset** : 205 questions sur 10 applications
- **Questions test√©es** : 20
- **LLM** : ollama/phi3
- **Embedding** : BAAI/bge-small-en-v1.5
- **Collection** : applications_v3 (33 chunks index√©s)

### M√©triques Techniques

| M√©trique | Valeur |
|----------|--------|
| Questions trait√©es | 20 |
| Succ√®s technique | 20/20 (100%) |
| Temps moyen | 92.5s |
| Tokens moyens | **700 tokens** ‚ö†Ô∏è |
| Temps total | 30.8 min |

### M√©triques Qualitatives

| M√©trique | Valeur | Objectif | Statut |
|----------|--------|----------|--------|
| **Similarit√© moyenne** | **36.6%** | 60-75% | ‚ùå Non atteint |
| R√©ponses correctes (‚â•80%) | 0/20 (0%) | 60-75% | ‚ùå |
| R√©ponses partielles (50-80%) | 10/20 (50%) | - | ‚ö†Ô∏è |
| R√©ponses incorrectes (<50%) | 10/20 (50%) | - | ‚ùå |

---

## Analyse des Probl√®mes

### ‚úÖ Ce qui fonctionne bien

1. **Retrieval (ChromaDB)** :
   - ‚úÖ Les bons chunks sont maintenant retrouv√©s
   - ‚úÖ Le mod√®le BAAI/bge-small-en-v1.5 am√©liore la qualit√© de recherche
   - ‚úÖ Les m√©tadonn√©es normalis√©es permettent un filtrage pr√©cis
   - ‚úÖ Distance vectorielle am√©lior√©e (1.03 ‚Üí 0.57)

### ‚ùå Ce qui pose probl√®me

1. **Generation (LLM phi3)** :
   - ‚ùå **Verbosit√© excessive** : 700 tokens en moyenne pour des r√©ponses attendues de 2-5 mots
   - ‚ùå **Non-respect du prompt strict** : Les contraintes "max 2 phrases" sont ignor√©es
   - ‚ùå **Ajout d'informations non demand√©es** : Le LLM contextualise au lieu de r√©pondre directement

**Exemples :**

**Question** : "Quel est le statut de 6Tzen ?"
**Attendu** : "En production"
**Obtenu** : "Le statut de 6Tzen est 'En production' (Chunk 2 - ID: 555671c28432d025)." *(70% similarit√©)*

**Question** : "Quel est l'ID de l'application 6Tzen ?"
**Attendu** : "1238"
**Obtenu** : "Question : Quel est l'ID de l'application 6Tzen ? R√©ponse : [Chunk 2 - ID: 555671c28432d025]" *(Ne contient PAS l'ID demand√©)*

### Probl√®me principal identifi√©

**Le retrieval fonctionne bien mais phi3 ne suit pas les instructions strictes.**

Le mod√®le :
- Ajoute des r√©f√©rences aux chunks
- Contextualise excessivement
- Ne suit pas la contrainte "max 2 phrases"
- Manque parfois l'information exacte demand√©e

---

## Comparaison Avant/Apr√®s

### Baseline (avant Phase 1)

*Note : Pas de r√©sultats baseline disponibles pour comparaison directe*

D'apr√®s les rapports pr√©c√©dents (`evaluation/PROPOSITION_AMELIORATION.md`) :
- Similarit√© estim√©e : ~8.5%
- Probl√®mes : Mauvais chunks retourn√©s, hallucinations

### Apr√®s Phase 1

- Similarit√© : 36.6%
- Am√©lioration : **+28 points** (estim√©)
- Retrieval : ‚úÖ Fonctionnel
- Generation : ‚ùå Probl√©matique

**Gain** : +330% de similarit√© (8.5% ‚Üí 36.6%)
**Statut** : Am√©lioration significative mais objectif non atteint

---

## Recommandations

### Option 1 : Changer de LLM (Recommand√©)

**Test llama3.2 au lieu de phi3** :

```bash
python -m dyag evaluate-rag evaluation/questions_10apps_rag.jsonl \
    --chroma-path ./chroma_db_10apps_v3 \
    --collection applications_v3 \
    --embedding-model BAAI/bge-small-en-v1.5 \
    --n-chunks 5 \
    --max-questions 20 \
    --llm-model llama3.2 \
    --output evaluation/results_phase1_llama32.json
```

**Justification** :
- llama3.2 suit g√©n√©ralement mieux les prompts stricts
- Meilleur √©quilibre concision/pr√©cision
- Pas de changement architectural n√©cessaire

**Impact attendu** : +20 √† +30 points de similarit√©

### Option 2 : Prompt Engineering Avanc√©

Modifier `src/dyag/rag_query.py` pour ajouter :
- Few-shot examples (2-3 exemples de r√©ponses parfaites)
- Format de sortie plus strict (JSON structur√©)
- P√©nalit√© explicite pour verbosit√©

**Impact attendu** : +10 √† +15 points de similarit√©

### Option 3 : Phase 2 - Reranking

Si l'Option 1 ne suffit pas :
- Ajouter `cross-encoder/ms-marco-MiniLM-L-6-v2` pour reranking
- Hybrid search (BM25 + embeddings)
- Am√©lioration attendue : +10 √† +20 points suppl√©mentaires

### Option 4 : Fine-tuning (Phase 3)

Derni√®re option si n√©cessaire :
- Fine-tuning l√©ger sur llama3.2-1b avec QLoRA 4-bit
- Dataset de 200+ questions/r√©ponses au format exact souhait√©
- Co√ªt : quelques heures de GPU, complexit√© √©lev√©e

---

## Plan d'Action Propos√©

### Imm√©diat (√† faire maintenant)

1. ‚úÖ **Tester avec llama3.2** au lieu de phi3
   - Relancer l'√©valuation avec le m√™me pipeline
   - Comparer les r√©sultats

2. ‚úÖ **Analyser les diff√©rences**
   - Utiliser `python -m dyag compare-rag` pour comparer phi3 vs llama3.2

### Court terme (si llama3.2 insuffisant)

3. **Am√©liorer le prompt** avec few-shot examples
4. **Tester avec qwen2.5-coder** qui est disponible localement

### Moyen terme (si similarit√© < 60%)

5. **Phase 2** : Impl√©menter le reranking
6. **Optimiser le chunking** (tester chunk_size 500 vs 1000)

---

## Fichiers Modifi√©s/Cr√©√©s

### Modifications

```
src/dyag/commands/create_rag.py        # M√©tadonn√©es normalis√©es + fixes
src/dyag/commands/evaluate_rag.py      # Support --embedding-model + dual format
src/dyag/rag_query.py                  # Prompt syst√®me strict
```

### Nouveaux fichiers

```
src/dyag/rag/comparison.py             # Logique de comparaison RAG
src/dyag/commands/compare_rag.py       # Commande CLI compare-rag
AMELIORATIONS_RAG_PHASE1.md            # Documentation Phase 1
RAPPORT_FINAL_PHASE1.md                # Ce rapport
evaluation/results_phase1_10apps.json  # R√©sultats √©valuation
evaluation/RAPPORT_PHASE1.md           # Rapport d√©taill√© auto-g√©n√©r√©
```

### Scripts utiles

```
scripts/reindex_rag_phase1.bat         # Windows
scripts/reindex_rag_phase1.sh          # Linux/Mac
```

---

## Conclusion

**Succ√®s de la Phase 1** :
- ‚úÖ Retrieval significativement am√©lior√©
- ‚úÖ Infrastructure solide pour futures am√©liorations
- ‚úÖ Nouveaux outils d'√©valuation et comparaison
- ‚ö†Ô∏è Similarit√© am√©lior√©e mais objectif non atteint (36.6% vs 60-75%)

**Probl√®me principal identifi√©** :
- ‚ùå phi3 ne respecte pas les contraintes de concision du prompt

**Prochaine √©tape recommand√©e** :
- üéØ **Tester llama3.2** imm√©diatement avant de passer √† la Phase 2

**Potentiel d'am√©lioration** :
- Avec llama3.2 : 55-65% similarit√© attendue
- Avec Phase 2 (reranking) : 70-80% similarit√© attendue
- Avec Phase 3 (fine-tuning) : 85-95% similarit√© attendue

---

## Commandes Utiles

### R√©-indexation compl√®te
```bash
python -m dyag markdown-to-rag applicationsIA_10apps.jsonl prepared/applications_10apps_chunks_v3.jsonl --chunk-size 1000
python -m dyag index-rag prepared/applications_10apps_chunks_v3.jsonl --chroma-path ./chroma_db_10apps_v3 --collection applications_v3 --embedding-model BAAI/bge-small-en-v1.5 --reset
```

### √âvaluation avec llama3.2
```bash
python -m dyag evaluate-rag evaluation/questions_10apps_rag.jsonl \
    --chroma-path ./chroma_db_10apps_v3 \
    --collection applications_v3 \
    --embedding-model BAAI/bge-small-en-v1.5 \
    --n-chunks 5 \
    --max-questions 20 \
    --output evaluation/results_llama32.json
```

### Comparaison
```bash
python -m dyag compare-rag evaluation/results_phase1_10apps.json evaluation/results_llama32.json
```

### G√©n√©ration de rapport
```bash
python -m dyag generate-evaluation-report evaluation/results_llama32.json --output evaluation/RAPPORT_LLAMA32.md
```

---

**Rapport g√©n√©r√© le 2025-12-24 par Claude Sonnet 4.5**
