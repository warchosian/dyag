# Workflow RAG - 10 Applications Minist√©rielles

**Date**: 2025-12-20
**Source**: `examples/test-mygusi/applicationsIA_mini_1-10.md`
**Objectif**: Cr√©er un syst√®me RAG complet et √©valuer ses performances

---

## üìã Vue d'ensemble

### Document Source
- **Fichier**: `applicationsIA_mini_1-10.md` (25 KB)
- **Contenu**: 10 applications du minist√®re de la transition √©cologique
- **Structure**: Markdown structur√© avec sections standardis√©es par application

### Applications Incluses
1. **6Tzen** - D√©mat√©rialisation des d√©marches transports routiers
2. **8 SINP** - Plateformes r√©gionales SINP habilit√©es
3. Et 8 autres applications...

---

## üéØ D√©marche Adopt√©e

### Phase 1: Cr√©ation de la Base RAG

#### Option Choisie: Pipeline Complet
Utilisation de la commande `markdown-to-rag` qui :
- ‚úÖ Pr√©pare le document (chunking intelligent)
- ‚úÖ G√©n√®re les embeddings
- ‚úÖ Indexe dans ChromaDB
- ‚úÖ Configure le syst√®me de requ√™te

#### Strat√©gie de Chunking
Pour ce type de document structur√©, nous utilisons :

**Strat√©gie recommand√©e : Semantic Chunking**
- D√©coupage bas√© sur la structure s√©mantique
- Pr√©servation du contexte applicatif
- Chaque chunk contient une section coh√©rente d'application

**Alternatives consid√©r√©es :**
- **Fixed-size** : Trop rigide, coupe les sections
- **Recursive** : Bon compromis mais moins optimal pour structure markdown
- **Markdown-aware** : Excellent pour ce cas d'usage

**Param√®tres :**
```bash
--chunk-size 800        # Taille suffisante pour une section compl√®te
--chunk-overlap 100     # Overlap pour pr√©server le contexte
--strategy semantic     # D√©coupage intelligent par s√©mantique
```

#### Base de Donn√©es
```
Nom: chroma_db_10apps
Emplacement: ./chroma_db_10apps/
Type: ChromaDB (vector database)
Embedding: sentence-transformers/all-MiniLM-L6-v2
Dimension: 384
```

---

## üîç G√©n√©ration des Questions/R√©ponses Attendues

### Approche 1: G√©n√©ration Manuelle (Recommand√©e pour 10 apps)

Cr√©ation d'un fichier `evaluation/questions_10apps.jsonl` avec :

```jsonl
{"question": "Quel est le statut de l'application 6Tzen ?", "expected_answer": "En production", "app_id": "1238", "category": "status"}
{"question": "Quels sont les domaines m√©tier de 6Tzen ?", "expected_answer": "Transports routiers", "app_id": "1238", "category": "domaine"}
{"question": "Quelle est la date de mise en production de 6Tzen ?", "expected_answer": "10/02/2020", "app_id": "1238", "category": "evenement"}
```

**Cat√©gories de questions :**
1. **Informations basiques** : Nom, ID, statut, port√©e
2. **Domaines m√©tier** : Domaines, th√©matiques
3. **Contacts** : Acteurs, contacts, √©quipes
4. **√âv√©nements** : Dates cl√©s, versions
5. **Relations** : Applications li√©es, donn√©es li√©es
6. **Techniques** : Technologies, h√©bergement, s√©curit√©
7. **Transversales** : Comparaisons entre applications

### Approche 2: G√©n√©ration Semi-Automatique

Utilisation d'un LLM (Claude/GPT) pour g√©n√©rer des questions :

```python
# Script de g√©n√©ration automatique
import anthropic
import json

def generate_questions_for_app(app_data, llm):
    prompt = f"""
    G√©n√®re 10 questions pertinentes sur cette application :

    {app_data}

    Format JSON avec : question, expected_answer, category
    """

    response = llm.complete(prompt)
    return json.loads(response)
```

**Avantages :**
- ‚úÖ Rapide pour g√©n√©rer beaucoup de questions
- ‚úÖ Diversit√© des formulations
- ‚úÖ Couvre tous les aspects

**Inconv√©nients :**
- ‚ö†Ô∏è N√©cessite validation humaine
- ‚ö†Ô∏è Risque de questions trop simples ou complexes

### Approche 3: Extraction Automatique

Extraction directe depuis le Markdown structur√© :

```python
# Extraction de paires Q/R depuis la structure
questions = [
    {
        "question": f"Quel est le statut de {app_name} ?",
        "answer": app["statut"],
        "source": "metadata"
    }
    # ... pour chaque champ structur√©
]
```

**Avantages :**
- ‚úÖ 100% fiable (donn√©es r√©elles)
- ‚úÖ Automatisable
- ‚úÖ R√©p√©table

**Inconv√©nients :**
- ‚ö†Ô∏è Questions peu naturelles
- ‚ö†Ô∏è Manque de vari√©t√©

---

## üìä Plan d'√âvaluation

### M√©triques de Performance

#### 1. M√©triques de R√©cup√©ration
- **Recall@k** : Proportion de chunks pertinents retrouv√©s
- **Precision@k** : Proportion de chunks pertinents parmi ceux retourn√©s
- **MRR (Mean Reciprocal Rank)** : Position moyenne du premier chunk pertinent

#### 2. M√©triques de R√©ponse
- **Exact Match** : R√©ponse exactement correcte
- **F1 Score** : Similarit√© entre r√©ponse g√©n√©r√©e et attendue
- **BLEU Score** : Qualit√© de la g√©n√©ration
- **Semantic Similarity** : Similarit√© s√©mantique (embeddings)

#### 3. M√©triques Utilisateur
- **Temps de r√©ponse** : < 2 secondes id√©al
- **Pertinence subjective** : √âvaluation humaine 1-5
- **Compl√©tude** : R√©ponse contient toutes les infos n√©cessaires

### Dataset d'√âvaluation

```
evaluation/
‚îú‚îÄ‚îÄ questions_10apps.jsonl          # Questions de test
‚îú‚îÄ‚îÄ questions_10apps_simple.jsonl   # Questions basiques
‚îú‚îÄ‚îÄ questions_10apps_complex.jsonl  # Questions complexes
‚îî‚îÄ‚îÄ questions_10apps_cross.jsonl    # Questions transversales
```

**Structure du dataset :**
```jsonl
{
  "id": "q001",
  "question": "Quel est le statut de 6Tzen ?",
  "expected_answer": "En production",
  "expected_chunks": ["chunk_6tzen_001"],
  "category": "status",
  "difficulty": "easy",
  "app_id": "1238"
}
```

### Sc√©narios de Test

#### Sc√©nario 1: Questions Simples Mono-Application
```
Q: "Quel est le statut de 6Tzen ?"
A attendue: "En production"
Chunks attendus: Section metadata de 6Tzen
```

#### Sc√©nario 2: Questions Complexes Multi-Sections
```
Q: "Quels sont les contacts et domaines m√©tier de 6Tzen ?"
A attendue: "Contact: 6Tzen Admin (6tzen-admin.ged.ds.msp.dnum.sg@developpement-durable.gouv.fr), Domaine: Transports routiers"
Chunks attendus: Section Contacts + Section Domaines m√©tier
```

#### Sc√©nario 3: Questions Transversales Multi-Applications
```
Q: "Quelles applications sont en production ?"
A attendue: Liste des applications avec statut "En production"
Chunks attendus: Sections metadata de toutes les apps
```

#### Sc√©nario 4: Questions de Comparaison
```
Q: "Quelle est la diff√©rence entre 6Tzen et 8 SINP ?"
A attendue: Comparaison des domaines, statuts, objectifs
Chunks attendus: Descriptions compl√®tes des deux apps
```

---

## üöÄ Commandes d'Ex√©cution

### √âtape 1: Cr√©ation de la Base RAG
```bash
python -m dyag markdown-to-rag \
  --input examples/test-mygusi/applicationsIA_mini_1-10.md \
  --output chroma_db_10apps \
  --chunk-size 800 \
  --chunk-overlap 100 \
  --strategy semantic \
  --verbose
```

### √âtape 2: G√©n√©ration des Questions
```bash
# Option manuelle
nano evaluation/questions_10apps.jsonl

# Option automatique (√† cr√©er)
python scripts/generate_questions.py \
  --input examples/test-mygusi/applicationsIA_mini_1-10.md \
  --output evaluation/questions_10apps.jsonl \
  --questions-per-app 5
```

### √âtape 3: √âvaluation
```bash
python -m dyag evaluate-rag \
  --db-path chroma_db_10apps \
  --questions evaluation/questions_10apps.jsonl \
  --output evaluation/results_10apps.json \
  --provider anthropic \
  --verbose
```

### √âtape 4: Requ√™tes Interactives
```bash
python -m dyag query-rag \
  --db-path chroma_db_10apps \
  --provider anthropic \
  --verbose
```

---

## üìà R√©sultats Attendus

### Performance Cible

| M√©trique | Objectif | Seuil Acceptable |
|----------|----------|------------------|
| Recall@5 | > 90% | > 80% |
| Precision@5 | > 85% | > 70% |
| Exact Match | > 60% | > 50% |
| Semantic Similarity | > 0.85 | > 0.75 |
| Temps de r√©ponse | < 1.5s | < 3s |

### Analyse Pr√©vue

1. **Par cat√©gorie de question**
   - Questions simples : performance √©lev√©e attendue
   - Questions complexes : peut n√©cessiter ajustement
   - Questions transversales : challenge principal

2. **Par type de chunk**
   - Sections structur√©es : excellente r√©cup√©ration
   - Descriptions longues : bon d√©coupage critique
   - Listes et √©num√©rations : attention aux coupures

3. **Optimisations potentielles**
   - Ajustement chunk-size si n√©cessaire
   - Test de diff√©rentes strat√©gies de chunking
   - Fine-tuning du nombre de chunks retourn√©s (top_k)

---

## üîÑ Workflow Complet

```mermaid
graph TD
    A[Document MD 10 apps] --> B[markdown-to-rag]
    B --> C[ChromaDB index√©e]
    C --> D[G√©n√©ration Questions]
    D --> E[Dataset √©valuation]
    E --> F[evaluate-rag]
    F --> G[R√©sultats + Analyse]
    G --> H{Performance OK?}
    H -->|Non| I[Ajustement param√®tres]
    I --> B
    H -->|Oui| J[Syst√®me RAG pr√™t]
    J --> K[query-rag interactif]
```

---

## üìù Livrables

1. **Base RAG fonctionnelle**
   - `chroma_db_10apps/` - Base vectorielle index√©e
   - Pr√™te pour requ√™tes en production

2. **Dataset d'√©valuation**
   - `evaluation/questions_10apps.jsonl` - 50+ questions
   - Couvre tous les types de questions

3. **Rapport de performance**
   - `evaluation/results_10apps.json` - M√©triques d√©taill√©es
   - Analyse par cat√©gorie

4. **Documentation**
   - Ce fichier - Workflow complet
   - Guide d'utilisation du syst√®me RAG

---

## üéì Approche Recommand√©e pour les Q/R

Pour ces 10 applications, je recommande une **approche hybride** :

### Phase 1: Extraction Automatique (Rapide)
G√©n√©rer automatiquement 5 questions basiques par application :
- Statut
- Domaine m√©tier
- Contact principal
- Date de modification
- Port√©e g√©ographique

**Total : 50 questions de base**

### Phase 2: G√©n√©ration Semi-Automatique (LLM)
Utiliser Claude/GPT pour g√©n√©rer 3 questions complexes par application :
- Questions multi-sections
- Questions d'analyse
- Questions de contexte

**Total : 30 questions complexes**

### Phase 3: Questions Manuelles Transversales
Cr√©er manuellement 20 questions inter-applications :
- Comparaisons
- Recherches par crit√®res
- Analyses globales

**Total : 20 questions transversales**

### Total Dataset: 100 questions
- ‚úÖ Diversit√© garantie
- ‚úÖ Tous niveaux de difficult√©
- ‚úÖ Validation de bout en bout

---

## üö¶ Prochaines √âtapes

1. ‚úÖ Cr√©er la base RAG avec `markdown-to-rag`
2. ‚è≥ G√©n√©rer le dataset de questions (approche hybride)
3. ‚è≥ √âvaluer les performances
4. ‚è≥ Ajuster les param√®tres si n√©cessaire
5. ‚è≥ Documenter les r√©sultats

---

**Document cr√©√© par**: Claude Code
**Version**: 1.0
**Projet**: DYAG v0.8.0
