# Sp√©cification: Commande `generate-questions`

**Version**: 1.0
**Date**: 2025-12-20
**Statut**: En d√©veloppement

---

## üéØ Objectif

G√©n√©rer automatiquement des paires question/r√©ponse depuis des documents Markdown structur√©s pour l'√©valuation de syst√®mes RAG.

---

## üìã Commande CLI

### Syntaxe
```bash
dyag generate-questions INPUT [OPTIONS]
```

### Arguments Positionnels

| Argument | Description | Type | Requis |
|----------|-------------|------|--------|
| `input` | Fichier Markdown source | Path | ‚úÖ |

### Options

| Option | Description | Type | D√©faut | Exemple |
|--------|-------------|------|--------|---------|
| `--output` | Fichier JSONL de sortie | Path | `{input}_questions.jsonl` | `questions.jsonl` |
| `--format` | Format de sortie | Choice | `rag` | `rag`, `finetuning`, `simple`, `all` |
| `--system-prompt` | Prompt syst√®me (format finetuning) | String | Auto | `"Tu es un assistant..."` |
| `--mode` | Mode de g√©n√©ration | Choice | `template` | `template`, `llm`, `hybrid` |
| `--questions-per-section` | Nombre de questions par section | Int | `3` | `5` |
| `--categories` | Cat√©gories de questions | List | `all` | `status,domains,contacts` |
| `--provider` | Provider LLM (mode llm/hybrid) | Choice | `anthropic` | `anthropic`, `openai`, `ollama` |
| `--model` | Mod√®le LLM sp√©cifique | String | Auto | `claude-3-5-sonnet-20241022` |
| `--difficulty` | Niveaux de difficult√© | List | `easy,medium` | `easy`, `medium`, `hard` |
| `--language` | Langue des questions | String | `fr` | `fr`, `en` |
| `--include-metadata` | Inclure m√©tadonn√©es dans output | Bool | `true` | - |
| `--validate` | Valider les questions g√©n√©r√©es | Bool | `true` | - |
| `--verbose` | Mode verbeux | Bool | `false` | - |

### Exemples

#### Mode Template (rapide, sans LLM)
```bash
dyag generate-questions examples/test-mygusi/applicationsIA_mini_1-10.md \
  --output evaluation/questions_10apps.jsonl \
  --mode template \
  --questions-per-section 3 \
  --categories status,domains,contacts,events \
  --verbose
```

#### Mode LLM (riche, n√©cessite API)
```bash
dyag generate-questions examples/test-mygusi/applicationsIA_mini_1-10.md \
  --output evaluation/questions_10apps_llm.jsonl \
  --mode llm \
  --provider anthropic \
  --model claude-3-5-sonnet-20241022 \
  --questions-per-section 5 \
  --difficulty easy,medium,hard \
  --verbose
```

#### Mode Hybride (meilleur √©quilibre)
```bash
dyag generate-questions examples/test-mygusi/applicationsIA_mini_1-10.md \
  --output evaluation/questions_10apps_hybrid.jsonl \
  --mode hybrid \
  --provider anthropic \
  --questions-per-section 4 \
  --verbose
```

---

## üîß Modes de G√©n√©ration

### Mode 1: Template (Sans LLM)

**Avantages**:
- ‚úÖ Rapide (pas d'appel API)
- ‚úÖ Gratuit
- ‚úÖ Reproductible √† 100%
- ‚úÖ Pas de d√©pendance externe

**Fonctionnement**:
1. Analyse la structure Markdown
2. Extrait les champs structur√©s
3. Applique des templates pr√©d√©finis

**Templates Pr√©d√©finis**:

```python
TEMPLATES = {
    "status": [
        "Quel est le statut de {app_name} ?",
        "L'application {app_name} est-elle en production ?",
        "Dans quel √©tat se trouve {app_name} ?",
    ],
    "domains": [
        "Quels sont les domaines m√©tier de {app_name} ?",
        "√Ä quels domaines m√©tier appartient {app_name} ?",
        "Dans quels domaines intervient {app_name} ?",
    ],
    "contacts": [
        "Qui est le contact principal pour {app_name} ?",
        "Comment contacter l'√©quipe de {app_name} ?",
        "Quels sont les contacts de {app_name} ?",
    ],
    "events": [
        "Quand {app_name} a-t-elle √©t√© mise en production ?",
        "Quelle est la date de cr√©ation de {app_name} ?",
        "Quels sont les √©v√©nements cl√©s de {app_name} ?",
    ],
    "description": [
        "Quelle est la description de {app_name} ?",
        "√Ä quoi sert {app_name} ?",
        "Quel est l'objectif de {app_name} ?",
    ],
    "websites": [
        "Quel est le site web de {app_name} ?",
        "O√π trouver {app_name} en ligne ?",
        "Quelle est l'URL de {app_name} ?",
    ],
    "related_apps": [
        "Quelles applications sont li√©es √† {app_name} ?",
        "Avec quelles autres applications {app_name} interagit-elle ?",
    ],
    "transversal": [
        "Quelles applications sont en production ?",
        "Combien d'applications concernent la biodiversit√© ?",
        "Quelles applications ont √©t√© modifi√©es en 2023 ?",
    ]
}
```

**Extraction de R√©ponses**:
```python
# Parsing du markdown structur√©
if line.startswith("**Statut:**"):
    answer = line.split(":", 1)[1].strip()

if line.startswith("## Domaines m√©tier"):
    # Collecte des domaines jusqu'√† la prochaine section
    domains = extract_list_items(section)
```

### Mode 2: LLM (Avec Intelligence Artificielle)

**Avantages**:
- ‚úÖ Questions naturelles et vari√©es
- ‚úÖ Comprend le contexte
- ‚úÖ G√©n√®re des questions complexes
- ‚úÖ Adapte la difficult√©

**Fonctionnement**:
1. D√©coupe le document par application
2. Pour chaque application, envoie au LLM avec prompt
3. Parse et valide les questions g√©n√©r√©es

**Prompt Template**:
```python
PROMPT_TEMPLATE = """Tu es un expert en g√©n√©ration de questions pour √©valuer des syst√®mes RAG.

Voici les informations sur une application:

{app_content}

G√©n√®re {n_questions} questions de test avec leurs r√©ponses attendues.

Cat√©gories √† couvrir: {categories}
Niveaux de difficult√©: {difficulty_levels}

Format de sortie (JSON):
{{
  "questions": [
    {{
      "question": "Question formul√©e naturellement",
      "expected_answer": "R√©ponse pr√©cise extraite du texte",
      "category": "status|domains|contacts|events|description|other",
      "difficulty": "easy|medium|hard",
      "reasoning": "Pourquoi cette question est pertinente"
    }}
  ]
}}

R√®gles:
1. Questions naturelles (comme un humain poserait)
2. R√©ponses extraites exactement du texte
3. Varier les formulations
4. Mix de questions simples et complexes
5. Questions qui n√©cessitent de combiner plusieurs sections
"""
```

**Validation**:
- V√©rifier que la r√©ponse existe dans le texte source
- V√©rifier la qualit√© de la formulation
- D√©tecter les doublons

### Mode 3: Hybride (Meilleur √âquilibre)

**Fonctionnement**:
1. G√©n√®re 50% des questions avec templates (rapide, fiable)
2. G√©n√®re 50% des questions avec LLM (riche, vari√©)
3. Fusionne et d√©duplique

**Avantages**:
- ‚úÖ √âquilibre co√ªt/qualit√©
- ‚úÖ Garantit une base solide (templates)
- ‚úÖ Ajoute de la richesse (LLM)

---

## üì¶ Formats de Sortie

La commande supporte **4 formats de sortie** pour diff√©rents cas d'usage :

### Format 1: `rag` (√âvaluation RAG)

**Usage**: √âvaluer les performances d'un syst√®me RAG

**Caract√©ristiques**:
- M√©tadonn√©es riches pour l'analyse
- Chunks attendus pour validation
- Cat√©gories et difficult√©
- Tra√ßabilit√© compl√®te

**Format**:
```jsonl
{"id": "q001", "question": "Quel est le statut de 6Tzen ?", "expected_answer": "En production", "category": "status", "difficulty": "easy", "app_name": "6Tzen", "app_id": "1238", "source_section": "metadata", "expected_chunks": ["chunk_6tzen_001"], "generated_by": "template"}
```

**Utilisation**:
```bash
dyag generate-questions apps.md --format rag --output questions_rag.jsonl

# Puis √©valuer
dyag evaluate-rag questions_rag.jsonl --collection my_rag_db
```

### Format 2: `finetuning` (Fine-tuning LLM)

**Usage**: Entra√Æner un mod√®le sur des donn√©es sp√©cifiques

**Caract√©ristiques**:
- Format OpenAI / Anthropic compatible
- Messages avec r√¥les (system, user, assistant)
- Optimis√© pour fine-tuning
- Prompt syst√®me personnalisable

**Format (Style OpenAI/Anthropic)**:
```jsonl
{"messages": [{"role": "system", "content": "Tu es un assistant expert sur les applications du minist√®re de la transition √©cologique. R√©ponds de mani√®re pr√©cise et factuelle."}, {"role": "user", "content": "Quel est le statut de 6Tzen ?"}, {"role": "assistant", "content": "L'application 6Tzen est actuellement en production. Elle a √©t√© mise en production le 10/02/2020 et sert √† la d√©mat√©rialisation des d√©marches des transports routiers."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant expert sur les applications du minist√®re de la transition √©cologique. R√©ponds de mani√®re pr√©cise et factuelle."}, {"role": "user", "content": "Quels sont les domaines m√©tier de 6Tzen ?"}, {"role": "assistant", "content": "6Tzen intervient dans le domaine des Transports routiers. Plus sp√©cifiquement, il s'agit de la d√©mat√©rialisation des proc√©dures administratives du registre des entreprises de transport par route."}]}
```

**Utilisation**:
```bash
# G√©n√©rer dataset fine-tuning
dyag generate-questions apps.md \
  --format finetuning \
  --output dataset_finetuning.jsonl \
  --system-prompt "Tu es un expert des applications minist√©rielles."

# Fine-tuner avec OpenAI
openai api fine_tunes.create \
  -t dataset_finetuning.jsonl \
  -m gpt-3.5-turbo

# Fine-tuner avec Anthropic (√† venir)
anthropic fine-tune create \
  --training-data dataset_finetuning.jsonl \
  --model claude-3-haiku
```

**Prompt Syst√®me par D√©faut**:
```
Tu es un assistant expert sur les applications du minist√®re de la transition √©cologique et solidaire.
Tu r√©ponds de mani√®re pr√©cise, factuelle et concise aux questions sur les applications,
leurs caract√©ristiques, leurs domaines d'intervention et leurs contacts.
Tes r√©ponses sont bas√©es uniquement sur les informations document√©es.
```

### Format 3: `simple` (Prompt/Completion)

**Usage**: Fine-tuning de mod√®les simples ou anciens

**Caract√©ristiques**:
- Format minimaliste
- Compatible mod√®les legacy
- Cl√© "prompt" / "completion"

**Format**:
```jsonl
{"prompt": "Quel est le statut de 6Tzen ?", "completion": "En production"}
{"prompt": "Quels sont les domaines m√©tier de 6Tzen ?", "completion": "Transports routiers"}
```

**Utilisation**:
```bash
dyag generate-questions apps.md --format simple --output dataset_simple.jsonl

# Fine-tuning avec format simple
openai api fine_tunes.create \
  -t dataset_simple.jsonl \
  -m davinci
```

### Format 4: `all` (Tous les formats)

**Usage**: G√©n√©rer tous les formats en une seule commande

**Comportement**:
Cr√©e 3 fichiers avec suffixes appropri√©s :
- `questions_rag.jsonl`
- `questions_finetuning.jsonl`
- `questions_simple.jsonl`

**Utilisation**:
```bash
dyag generate-questions apps.md \
  --format all \
  --output questions

# G√©n√®re:
# - questions_rag.jsonl
# - questions_finetuning.jsonl
# - questions_simple.jsonl
```

---

## üìä Tableau Comparatif des Formats

| Format | RAG Eval | Fine-tuning | M√©tadonn√©es | Taille | Compatible |
|--------|----------|-------------|-------------|--------|------------|
| `rag` | ‚úÖ | ‚ùå | ‚úÖ‚úÖ‚úÖ | Grande | dyag evaluate-rag |
| `finetuning` | ‚ùå | ‚úÖ‚úÖ | ‚ö†Ô∏è Minimales | Moyenne | OpenAI, Anthropic, Hugging Face |
| `simple` | ‚ùå | ‚úÖ | ‚ùå | Petite | Legacy models |
| `all` | ‚úÖ | ‚úÖ | ‚úÖ | Grande (x3) | Tous |

---

## üìÑ Format de Sortie - D√©tails

### Structure
```jsonl
{"id": "q001", "question": "Quel est le statut de 6Tzen ?", "expected_answer": "En production", "category": "status", "difficulty": "easy", "app_name": "6Tzen", "app_id": "1238", "source_section": "metadata", "generated_by": "template"}
{"id": "q002", "question": "Quels domaines m√©tier couvre 6Tzen ?", "expected_answer": "Transports routiers", "category": "domains", "difficulty": "easy", "app_name": "6Tzen", "app_id": "1238", "source_section": "domaines_metier", "generated_by": "template"}
{"id": "q003", "question": "Pourquoi 6Tzen a-t-elle √©t√© cr√©√©e et quels sont ses avantages pour l'usager ?", "expected_answer": "6Tzen a √©t√© cr√©√©e dans le cadre du programme gouvernemental de simplification. Les avantages incluent: gain de temps, suivi simplifi√©, instruction facilit√©e, et diminution des d√©lais de traitement.", "category": "description", "difficulty": "medium", "app_name": "6Tzen", "app_id": "1238", "source_section": "description", "generated_by": "llm"}
```

### Champs Obligatoires

| Champ | Type | Description | Exemple |
|-------|------|-------------|---------|
| `id` | String | Identifiant unique | `"q001"` |
| `question` | String | Question formul√©e | `"Quel est le statut ?"` |
| `expected_answer` | String | R√©ponse attendue | `"En production"` |
| `category` | String | Cat√©gorie de question | `"status"` |
| `difficulty` | String | Niveau de difficult√© | `"easy"` |

### Champs Optionnels

| Champ | Type | Description | Exemple |
|-------|------|-------------|---------|
| `app_name` | String | Nom de l'application | `"6Tzen"` |
| `app_id` | String | ID de l'application | `"1238"` |
| `source_section` | String | Section source | `"metadata"` |
| `generated_by` | String | Mode de g√©n√©ration | `"template"` / `"llm"` |
| `reasoning` | String | Justification (LLM) | `"Teste la compr√©hension..."` |
| `expected_chunks` | List | IDs chunks attendus | `["chunk_001"]` |
| `metadata` | Object | M√©tadonn√©es diverses | `{}` |

---

## üèóÔ∏è Architecture d'Impl√©mentation

### Structure de Fichiers

```
src/dyag/commands/
‚îú‚îÄ‚îÄ generate_questions.py       # Commande CLI principale
‚îî‚îÄ‚îÄ question_generators/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ base.py                 # Classe abstraite BaseGenerator
    ‚îú‚îÄ‚îÄ template_generator.py   # Mode template
    ‚îú‚îÄ‚îÄ llm_generator.py        # Mode LLM
    ‚îú‚îÄ‚îÄ hybrid_generator.py     # Mode hybride
    ‚îú‚îÄ‚îÄ parser.py               # Parsing Markdown structur√©
    ‚îú‚îÄ‚îÄ validator.py            # Validation des questions
    ‚îî‚îÄ‚îÄ templates.py            # Templates de questions
```

### Classes Principales

```python
# base.py
class BaseQuestionGenerator(ABC):
    @abstractmethod
    def generate(self, content: dict) -> List[Question]:
        pass

    def validate(self, questions: List[Question]) -> List[Question]:
        pass

# template_generator.py
class TemplateQuestionGenerator(BaseQuestionGenerator):
    def __init__(self, templates: dict, categories: List[str]):
        self.templates = templates
        self.categories = categories

    def generate(self, content: dict) -> List[Question]:
        questions = []
        for category in self.categories:
            if category in content:
                questions.extend(
                    self._generate_from_template(category, content[category])
                )
        return questions

# llm_generator.py
class LLMQuestionGenerator(BaseQuestionGenerator):
    def __init__(self, provider: str, model: str):
        self.llm = create_llm_provider(provider, model)

    def generate(self, content: dict) -> List[Question]:
        prompt = self._build_prompt(content)
        response = self.llm.complete(prompt)
        questions = self._parse_response(response)
        return questions

# parser.py
class MarkdownParser:
    def parse_application(self, markdown: str) -> dict:
        """Extrait les infos structur√©es d'une application"""
        pass

    def extract_applications(self, markdown: str) -> List[dict]:
        """Extrait toutes les applications du document"""
        pass
```

### Flux d'Ex√©cution

```mermaid
graph TD
    A[Input MD] --> B[Parser]
    B --> C{Mode?}
    C -->|Template| D[TemplateGenerator]
    C -->|LLM| E[LLMGenerator]
    C -->|Hybrid| F[HybridGenerator]
    D --> G[Validator]
    E --> G
    F --> G
    G --> H[Output JSONL]
```

---

## üîå Int√©gration MCP

### D√©claration de l'Outil

```python
# src/dyag/mcp_server.py

@mcp.tool()
async def generate_questions(
    input_file: str,
    output_file: str | None = None,
    mode: Literal["template", "llm", "hybrid"] = "template",
    questions_per_section: int = 3,
    categories: list[str] | None = None,
    provider: str = "anthropic",
    model: str | None = None,
    difficulty: list[str] | None = None,
) -> dict:
    """
    G√©n√®re des paires question/r√©ponse pour l'√©valuation RAG

    Args:
        input_file: Fichier Markdown source
        output_file: Fichier JSONL de sortie (optionnel)
        mode: Mode de g√©n√©ration (template/llm/hybrid)
        questions_per_section: Nombre de questions par section
        categories: Cat√©gories √† g√©n√©rer (None = toutes)
        provider: Provider LLM (anthropic/openai/ollama)
        model: Mod√®le LLM sp√©cifique
        difficulty: Niveaux de difficult√©

    Returns:
        dict: {
            "questions_generated": int,
            "output_file": str,
            "mode": str,
            "summary": dict
        }
    """
    from dyag.commands.generate_questions import generate_questions_from_markdown

    result = generate_questions_from_markdown(
        input_path=input_file,
        output_path=output_file,
        mode=mode,
        questions_per_section=questions_per_section,
        categories=categories or ["all"],
        provider=provider,
        model=model,
        difficulty=difficulty or ["easy", "medium"],
        verbose=False
    )

    return result
```

---

## ‚úÖ Crit√®res de Validation

### Validation des Questions

1. **Syntaxe**:
   - Question se termine par `?`
   - Longueur > 10 caract√®res
   - Pas de caract√®res sp√©ciaux invalides

2. **S√©mantique**:
   - La r√©ponse existe dans le texte source
   - La question est pertinente pour la cat√©gorie
   - Pas de questions ambigu√´s

3. **Qualit√©** (mode LLM):
   - Formulation naturelle
   - Pas de r√©p√©titions
   - Diversit√© des formulations

### M√©triques de Qualit√©

```python
quality_metrics = {
    "total_questions": 100,
    "valid_questions": 98,
    "invalid_questions": 2,
    "by_category": {
        "status": 15,
        "domains": 12,
        "contacts": 10,
        # ...
    },
    "by_difficulty": {
        "easy": 50,
        "medium": 30,
        "hard": 20
    },
    "by_generator": {
        "template": 60,
        "llm": 40
    }
}
```

---

## üß™ Tests

### Tests Unitaires

```python
# tests/unit/commands/test_generate_questions.py

def test_template_generator_basic():
    """Test g√©n√©ration basique avec templates"""
    generator = TemplateQuestionGenerator(templates, ["status"])
    content = {"status": "En production", "app_name": "6Tzen"}
    questions = generator.generate(content)
    assert len(questions) > 0
    assert "6Tzen" in questions[0].question

def test_llm_generator_with_mock():
    """Test g√©n√©ration LLM avec mock"""
    mock_llm = MockLLMProvider()
    generator = LLMQuestionGenerator(mock_llm)
    content = {"description": "...", "app_name": "Test"}
    questions = generator.generate(content)
    assert len(questions) > 0

def test_validator():
    """Test validation des questions"""
    validator = QuestionValidator()
    valid_q = Question("Quel est le statut ?", "En production")
    invalid_q = Question("Test", "")  # Pas de ?

    assert validator.validate(valid_q) == True
    assert validator.validate(invalid_q) == False
```

### Tests d'Int√©gration

```bash
# Test end-to-end
python -m dyag generate-questions \
  examples/test-mygusi/applicationsIA_mini_1-10.md \
  --output /tmp/test_questions.jsonl \
  --mode template \
  --verbose

# V√©rifier output
python -m pytest tests/integration/test_generate_questions.py
```

---

## üìö Documentation

### README Section

````markdown
### G√©n√©ration de Questions pour RAG

G√©n√©rez automatiquement des questions/r√©ponses pour √©valuer votre syst√®me RAG.

#### Utilisation de Base

```bash
# Mode template (rapide, sans API)
dyag generate-questions document.md --mode template

# Mode LLM (riche, n√©cessite API)
dyag generate-questions document.md --mode llm --provider anthropic

# Mode hybride (recommand√©)
dyag generate-questions document.md --mode hybrid
```

#### Exemples

```bash
# G√©n√©rer 5 questions par section, toutes cat√©gories
dyag generate-questions apps.md \
  --output questions.jsonl \
  --questions-per-section 5

# Seulement status et domains, difficult√© facile
dyag generate-questions apps.md \
  --categories status,domains \
  --difficulty easy

# Avec Claude pour g√©n√©ration avanc√©e
dyag generate-questions apps.md \
  --mode llm \
  --provider anthropic \
  --model claude-3-5-sonnet-20241022
```
````

---

## üöÄ Prochaines √âtapes

1. ‚úÖ Sp√©cification √©crite
2. ‚è≥ Impl√©mentation:
   - `parser.py` - Parsing Markdown
   - `template_generator.py` - Mode template
   - `llm_generator.py` - Mode LLM
   - `hybrid_generator.py` - Mode hybride
   - `validator.py` - Validation
   - `generate_questions.py` - CLI command
3. ‚è≥ Tests unitaires
4. ‚è≥ Int√©gration MCP
5. ‚è≥ Documentation
6. ‚è≥ Tests sur applicationsIA_mini_1-10.md

---

**Version**: 1.0
**Auteur**: Claude Code
**Projet**: DYAG v0.8.0
